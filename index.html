<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
    
<script src="https://raw.githubusercontent.com/HubSpot/pace/v1.0.2/pace.min.js"></script>

    
<link rel="stylesheet" href="https://github.com/HubSpot/pace/raw/master/themes/orange/pace-theme-flash.css">

  
  

  <!-- PACE Progress Bar START -->

  
  <title>Yujie&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  
  <meta name="description" content="欢迎和我一起进步！分享生活！分享热爱">
<meta property="og:type" content="website">
<meta property="og:title" content="Yujie&#39;s Blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Yujie&#39;s Blog">
<meta property="og:description" content="欢迎和我一起进步！分享生活！分享热爱">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Chen Yujie">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Yujie&#39;s Blog" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  
<link rel="stylesheet" href="/css/style.css">


  
<script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>


  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.2/css/bootstrap.min.css" >
  <link rel="stylesheet" href="/css/hiero.css" >
  <link rel="stylesheet" href="/css/glyphs.css" >
  
    <link rel="stylesheet" href="/css/vdonate.css" >
  

  <!-- Custom CSS -->
  
<link rel="stylesheet" href="/css/my.css">

  <!-- Google Adsense -->
  
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-0123456789ABCDEF",
          enable_page_level_ads: true
      });
  </script>
  
<meta name="generator" content="Hexo 6.3.0"></head>

<script>
var themeMenus = {};

  themeMenus["/"] = "首页"; 

  themeMenus["/archives"] = "归档"; 

  themeMenus["/categories"] = "分类"; 

  themeMenus["/tags"] = "标签"; 

  themeMenus["/about"] = "关于"; 

</script>


  <body>


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="Yujie&#39;s Blog" rel="home"> Yujie&#39;s Blog </a>
            
          </h1>

          
            <div class="site-description">欢迎和我一起进步！分享生活！分享热爱</div>
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">首页</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">归档</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">分类</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">标签</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">关于</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>


  <div id="originBgDiv" style="background: #fff; width: 100%;">

      <div style="max-height:600px; overflow: hidden;  display: flex; display: -webkit-flex; align-items: center;">
        <img id="originBg" width="100%" alt="" src="">
      </div>

  </div>

  <script>
  function setAboutIMG(){
      var imgUrls = "css/images/pose.jpg,https://source.unsplash.com/collection/954550/1920x1080".split(",");
      var random = Math.floor((Math.random() * imgUrls.length ));
      if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
        document.getElementById("originBg").src=imgUrls[random];
      } else {
        document.getElementById("originBg").src='/' + imgUrls[random];
      }
  }
  bgDiv=document.getElementById("originBgDiv");
  if(location.pathname.match('about')){
    setAboutIMG();
    bgDiv.style.display='block';
  }else{
    bgDiv.style.display='none';
  }
  </script>



  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main">
  
    <article id="post-Untitled"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
    <div class="article-meta">
      
	Posted on <a href="/2023/08/09/Untitled/" class="article-date">
	  <time datetime="2023-08-09T01:19:40.626Z" itemprop="datePublished">八月 9, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="entry-meta entry-footer">
      
      
      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-LLM相关最新进展"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2023/08/08/LLM%E7%9B%B8%E5%85%B3%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95/">LLM相关进展</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2023/08/08/LLM%E7%9B%B8%E5%85%B3%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95/" class="article-date">
	  <time datetime="2023-08-08T08:23:51.858Z" itemprop="datePublished">八月 8, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>跟踪大语言模型相关的最新进展</p>
<h1 id="LLM类型"><a href="#LLM类型" class="headerlink" title="LLM类型"></a>LLM类型</h1><h2 id="中文大模型"><a href="#中文大模型" class="headerlink" title="中文大模型"></a>中文大模型</h2><h3 id="通义千问"><a href="#通义千问" class="headerlink" title="通义千问"></a>通义千问</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>通义千问-7B（Qwen-7B） 是阿里云研发的通义千问大模型系列的70亿参数规模的模型。Qwen-7B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。同时，在Qwen-7B的基础上，我们使用对齐机制打造了基于大语言模型的AI助手Qwen-7B-Chat。Qwen-7B系列模型的特点包括：</p>
<ol>
<li><strong>大规模高质量预训练数据</strong>：我们使用了超过2.2万亿token的自建大规模预训练数据集进行语言模型的预训练。数据集包括文本和代码等多种数据类型，覆盖通用领域和专业领域。</li>
<li><strong>优秀的模型性能</strong>：相比同规模的开源模型，Qwen-7B在多个评测数据集上具有显著优势，甚至超出12-13B等更大规模的模型。评测评估的能力范围包括自然语言理解与生成、数学运算解题、代码生成等。</li>
<li><strong>更好地支持多语言</strong>：基于更大词表的分词器在分词上更高效，同时它对其他语言表现更加友好。用户可以在Qwen-7B的基础上更方便地训练特定语言的7B语言模型。</li>
<li><strong>8K的上下文长度</strong>：Qwen-7B及Qwen-7B-Chat均能支持8K的上下文长度, 允许用户输入更长的prompt。</li>
<li><strong>支持插件调用</strong>：Qwen-7B-Chat针对插件调用相关的对齐数据做了特定优化，当前模型能有效调用插件以及升级为Agent。</li>
</ol>
<h5 id="模型基座"><a href="#模型基座" class="headerlink" title="模型基座"></a><strong>模型基座</strong></h5><p>基于transformer的纯解码器语言模型，其架构类似于LLaMA系列模型。和标准transformer不同之处在于：</p>
<blockquote>
<ol>
<li>using <strong><u>untied embedding</u></strong></li>
<li>using <strong><u>rotary positional embedding</u></strong></li>
<li><strong><u>no biases</u></strong> except for QKV in attention</li>
<li><strong><u>RMSNorm</u></strong> instead of LayerNorm</li>
<li><strong><u>SwiGLU</u></strong> instead of ReLU</li>
<li>adopting <strong><u>flash attention</u></strong> to accelerate training.</li>
</ol>
<p>The model has 32 layers, the embedding dimension is 4096, and the number of attention heads is 32.</p>
</blockquote>
<h5 id="预训练Qwen-7B"><a href="#预训练Qwen-7B" class="headerlink" title="预训练Qwen-7B"></a><strong>预训练Qwen-7B</strong></h5><blockquote>
<p><strong>预训练</strong>：超过2.2万亿个token上进行预训练，从公开可用的数据中获得2048个上下文长度，涵盖一般和专业领域，重点是英语和汉语</p>
<p><strong>预训练数据</strong>处理：我们的数据包括来自公开来源的混合数据，主要包括网络文档和代码文件。此外，数据是多语种的，大部分是英语和汉语。我们努力使用一系列模型来排除低质量或被认为不适合预训练的数据，例如 NSFW 内容。对于数学推理，我们包括来自<a target="_blank" rel="noopener" href="https://github.com/ofa-sys/gsm8k-ScRel"> gsm8k-scRel </a>的 RFT 数据。最终数据进行了全局模糊重复（global fuzzy deduplication）数据删除。通过大量的消融实验，优化了预训练语料库的组合。</p>
<p><strong>tokenization</strong>：词表 151,851 tokens，它首先考虑了汉语、英语和代码数据的高效编码，而且对多语言更加友好，使用户可以直接提高某些语言的能力，而无需扩充词汇量。它按对数字进行分割（单个），并调用 <a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken">tiktoken</a>  tokenizer库以进行有效的tokenization。tokenization后的数据总量超过2.2万亿个token。</p>
<p>我们随机选择每种语言的100万个文档语料库来测试和比较不同模型的编码压缩率(使用支持100种语言的 XLM-R 作为基值1，图中未显示)。可以看出，Qwen-7B 在保证汉语、英语和代码的高效解码的同时，还能在其他多种语言(如 泰语th、希伯来语he、阿拉伯语ar、韩语ko、越南语vi、日语ja、土耳其语tr、印尼语id、波兰语pl、俄语ru、荷兰语nl、葡萄牙语pt、意大利语it、德语de、西班牙语es、法语fr等)中获得较高的压缩率，使模型具有很强的可扩展性，以及这些语言的高训练和推理效率。</p>
<p><strong>训练细节</strong>: </p>
<blockquote>
<p>The model is trained using the AdamW optimizer, with<br>$$<br>\beta_1&#x3D;0.9, \beta_2&#x3D;0.95, \epsilon&#x3D;10^{-6}<br>$$<br>. The sequence length is 2048, and the batch size is 2048, which means each optimization step accumulates over 4 million tokens. We use a cosine learning rate schedule, with a warm-up of 2000 steps, a peak learning rate of 3×10^−4, and a minimum learning rate of 10% of the peak learning rate. We use a weight decay of 0.1 and gradient clipping of 1.0. The training adopts mixed precision training with <code>bfloat16</code>.</p>
</blockquote>
<p>评估：</p>
<p>【1】World knowledge（世界知识）</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.08322"> <strong>C-Eval</strong> </a>是一个通用的评估基准，用于测试预训练的中文模型的常识能力。它涵盖了四个主要方向的52个学科: 人文、社会科学、科学、技术、工程和其他专业（humanities, social sciences, STEM, and other specialties）。根据标准实践，以validation样本作为少样本prompt的来源，对 Qwen-7B 预训练模型的5-shot验证集和测试集精度进行了评估。</p>
<p><strong>MMLU</strong> 是目前评价英语理解能力最公认的基准之一，涵盖了不同学术领域和难度水平的57个子任务。</p>
<p>【2】Coding（编程）</p>
<p>**<a target="_blank" rel="noopener" href="https://github.com/openai/human-eval">HumanEval</a>**：Pass@1</p>
<p>【3】Math（数学）</p>
<p><a target="_blank" rel="noopener" href="https://github.com/openai/grade-school-math">GSM8K</a> (8-shot)：Accuracy</p>
<p>【4】Natural language processing 自然语言处理（翻译）</p>
<p>WMT22 zh-en and en-zh (5-shot BLEU)</p>
<p>【5】Long-context inference 长文本外推</p>
</blockquote>
<h5 id="微调Qwen-7B-Chat"><a href="#微调Qwen-7B-Chat" class="headerlink" title="微调Qwen-7B-Chat"></a><strong>微调Qwen-7B-Chat</strong></h5><blockquote>
<p>**对齐数据 ** :<br>这些数据包括常见的指令风格的对话，以及涉及大量标注工作的面向安全和服务的数据。<br>– 指令数据包括广泛的能力，如写作，问题回答，头脑风暴和计划，内容理解，总结，自然语言处理和编码。<br>– 安全数据试图防止模型生成有害和不适当的内容。<br>– 服务数据尝试使用特定的会话模式来增强模型，这些模式可以被解析以调用和合并外部系统。</p>
<p><strong>数据格式</strong> :<br>由于数据由会话轮数组成，我们使用<a target="_blank" rel="noopener" href="https://github.com/openai/openai-python/blob/main/ChatML.md"> chatML </a>格式将它们排列成文本，这是一种元语言，既可以描述元数据(例如，roles) ，也可以描述每轮的内容。<br>目前，现有的角色包括系统、用户和助手(system, user, and assistant)。</p>
<p><strong>模型训练细节：</strong></p>
<p>使用因果语言建模目标（causal language modeling）对模型进行微调，除了user轮次内容中的token。</p>
<p>该模型使用 AdamW 优化器进行训练，其中 $beta _ 1 &#x3D; 0.9，beta _ 2 &#x3D; 0.95，epsilon &#x3D; 10 ^ {-6} $。</p>
<p>序列长度限制为2048，批量大小为128。</p>
<p>该模型经过4000个步骤的训练，在前1430个步骤中，学习速率被加热到 $1乘以10 ^ { -5} $。</p>
<p>我们使用0.1的权重衰减，0.1的辍学，和1.0的梯度裁剪。</p>
<p><strong>评估：</strong></p>
<p>【1】World knowledge</p>
<p>由于微调使用的数据集比预训练小得多，人类对世界知识的理解可能有限，我们还使用 C-Eval 和 MMLU 以zero-shot和生成的方式评估 Qwen-7B-Chat 的世界知识。</p>
<p>C-Eval validation set：zero-shot accuracy</p>
<p>MMLU：zero-shot accuracy</p>
<p>【2】Coding</p>
<p><a target="_blank" rel="noopener" href="https://github.com/openai/human-eval">HumanEval</a>：zero-shot Pass@1</p>
<p>【3】Math</p>
<p>GSM8K：accuracy</p>
<p>【4】Service</p>
<p>Tool Selection (Acc.↑)、Tool Input (Rouge-L↑) 、 False Positive Error↓  </p>
</blockquote>
<h4 id="评测表现"><a href="#评测表现" class="headerlink" title="评测表现"></a>评测表现</h4><p>Qwen-7B在多个全面评估自然语言理解与生成、数学运算解题、代码生成等能力的评测数据集上，包括MMLU、C-Eval、GSM8K、HumanEval、WMT22等，均超出了同规模大语言模型的表现，甚至超出了如12-13B参数等更大规模的语言模型。</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>MMLU</th>
<th>C-Eval</th>
<th>GSM8K</th>
<th>HumanEval</th>
<th>WMT22 (en-zh)</th>
</tr>
</thead>
<tbody><tr>
<td>LLaMA-7B</td>
<td>35.1</td>
<td>-</td>
<td>11.0</td>
<td>10.5</td>
<td>8.7</td>
</tr>
<tr>
<td>LLaMA 2-7B</td>
<td>45.3</td>
<td>-</td>
<td>14.6</td>
<td>12.8</td>
<td>17.9</td>
</tr>
<tr>
<td>Baichuan-7B</td>
<td>42.3</td>
<td>42.8</td>
<td>9.7</td>
<td>9.2</td>
<td>26.6</td>
</tr>
<tr>
<td>ChatGLM2-6B</td>
<td>47.9</td>
<td>51.7</td>
<td>32.4</td>
<td>9.2</td>
<td>-</td>
</tr>
<tr>
<td>InternLM-7B</td>
<td>51.0</td>
<td>52.8</td>
<td>31.2</td>
<td>10.4</td>
<td>14.8</td>
</tr>
<tr>
<td>Baichuan-13B</td>
<td>51.6</td>
<td>53.6</td>
<td>26.6</td>
<td>12.8</td>
<td>30.0</td>
</tr>
<tr>
<td>LLaMA-13B</td>
<td>46.9</td>
<td>35.5</td>
<td>17.8</td>
<td>15.8</td>
<td>12.0</td>
</tr>
<tr>
<td>LLaMA 2-13B</td>
<td>54.8</td>
<td>-</td>
<td>28.7</td>
<td>18.3</td>
<td>24.2</td>
</tr>
<tr>
<td>ChatGLM2-12B</td>
<td>56.2</td>
<td><strong>61.6</strong></td>
<td>40.9</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><strong>Qwen-7B</strong></td>
<td><strong>56.7</strong></td>
<td>59.6</td>
<td><strong>51.6</strong></td>
<td><strong>24.4</strong></td>
<td><strong>30.6</strong></td>
</tr>
</tbody></table>
<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><p>在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<p>如果你的显卡支持fp16或bf16精度，我们还推荐安装<a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention">flash-attention</a>来提高你的运行效率以及降低显存占用。(<strong>flash-attention只是可选项，不安装也可正常运行该项目</strong>)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention</span><br><span class="line">cd flash-attention &amp;&amp; pip install .</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">下方安装可选，安装可能比较缓慢。</span></span><br><span class="line">pip install csrc/layer_norm</span><br><span class="line">pip install csrc/rotary</span><br></pre></td></tr></table></figure>

<p>接下来可以开始使用Transformers或者ModelScope来使用我们的模型。</p>
<h5 id="🤗-Transformers"><a href="#🤗-Transformers" class="headerlink" title="🤗 Transformers"></a>🤗 Transformers</h5><p>如希望使用Qwen-7B-chat进行推理，所需要写的只是如下所示的数行代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> transformers.generation <span class="keyword">import</span> GenerationConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请注意：分词器默认行为已更改为默认关闭特殊token攻击防护。</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;Qwen/Qwen-7B-Chat&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开bf16精度，A100、H100、RTX3060、RTX3070等显卡建议启用以节省显存</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained(&quot;Qwen/Qwen-7B-Chat&quot;, device_map=&quot;auto&quot;, trust_remote_code=True, bf16=True).eval()</span></span><br><span class="line"><span class="comment"># 打开fp16精度，V100、P100、T4等显卡建议启用以节省显存</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained(&quot;Qwen/Qwen-7B-Chat&quot;, device_map=&quot;auto&quot;, trust_remote_code=True, fp16=True).eval()</span></span><br><span class="line"><span class="comment"># 使用CPU进行推理，需要约32GB内存</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained(&quot;Qwen/Qwen-7B-Chat&quot;, device_map=&quot;cpu&quot;, trust_remote_code=True).eval()</span></span><br><span class="line"><span class="comment"># 默认使用自动模式，根据设备自动选择精度</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;Qwen/Qwen-7B-Chat&quot;</span>, device_map=<span class="string">&quot;auto&quot;</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可指定不同的生成长度、top_p等相关超参</span></span><br><span class="line">model.generation_config = GenerationConfig.from_pretrained(<span class="string">&quot;Qwen/Qwen-7B-Chat&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一轮对话 1st dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">&quot;你好&quot;</span>, history=<span class="literal">None</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 你好！很高兴为你提供帮助。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二轮对话 2nd dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">&quot;给我讲一个年轻人奋斗创业最终取得成功的故事。&quot;</span>, history=history) </span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 这是一个关于一个年轻人奋斗创业最终取得成功的故事。</span></span><br><span class="line"><span class="comment"># 故事的主人公叫李明，他来自一个普通的家庭，父母都是普通的工人。从小，李明就立下了一个目标：要成为一名成功的企业家。</span></span><br><span class="line"><span class="comment"># 为了实现这个目标，李明勤奋学习，考上了大学。在大学期间，他积极参加各种创业比赛，获得了不少奖项。他还利用课余时间去实习，积累了宝贵的经验。</span></span><br><span class="line"><span class="comment"># 毕业后，李明决定开始自己的创业之路。他开始寻找投资机会，但多次都被拒绝了。然而，他并没有放弃。他继续努力，不断改进自己的创业计划，并寻找新的投资机会。</span></span><br><span class="line"><span class="comment"># 最终，李明成功地获得了一笔投资，开始了自己的创业之路。他成立了一家科技公司，专注于开发新型软件。在他的领导下，公司迅速发展起来，成为了一家成功的科技企业。</span></span><br><span class="line"><span class="comment"># 李明的成功并不是偶然的。他勤奋、坚韧、勇于冒险，不断学习和改进自己。他的成功也证明了，只要努力奋斗，任何人都有可能取得成功。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三轮对话 3rd dialogue turn</span></span><br><span class="line">response, history = model.chat(tokenizer, <span class="string">&quot;给这个故事起一个标题&quot;</span>, history=history)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"><span class="comment"># 《奋斗创业：一个年轻人的成功之路》</span></span><br></pre></td></tr></table></figure>

<p>运行Qwen-7B同样非常简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> transformers.generation <span class="keyword">import</span> GenerationConfig</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;Qwen/Qwen-7B&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开bf16精度，A100、H100、RTX3060、RTX3070等显卡建议启用以节省显存</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained(&quot;Qwen/Qwen-7B&quot;, device_map=&quot;auto&quot;, trust_remote_code=True, bf16=True).eval()</span></span><br><span class="line"><span class="comment"># 打开fp16精度，V100、P100、T4等显卡建议启用以节省显存</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained(&quot;Qwen/Qwen-7B&quot;, device_map=&quot;auto&quot;, trust_remote_code=True, fp16=True).eval()</span></span><br><span class="line"><span class="comment"># 使用CPU进行推理，需要约32GB内存</span></span><br><span class="line"><span class="comment"># model = AutoModelForCausalLM.from_pretrained(&quot;Qwen/Qwen-7B&quot;, device_map=&quot;cpu&quot;, trust_remote_code=True).eval()</span></span><br><span class="line"><span class="comment"># 默认使用自动模式，根据设备自动选择精度</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;Qwen/Qwen-7B&quot;</span>, device_map=<span class="string">&quot;auto&quot;</span>, trust_remote_code=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可指定不同的生成长度、top_p等相关超参</span></span><br><span class="line">model.generation_config = GenerationConfig.from_pretrained(<span class="string">&quot;Qwen/Qwen-7B&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">inputs = tokenizer(<span class="string">&#x27;蒙古国的首都是乌兰巴托（Ulaanbaatar）\n冰岛的首都是雷克雅未克（Reykjavik）\n埃塞俄比亚的首都是&#x27;</span>, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line">inputs = inputs.to(model.device)</span><br><span class="line">pred = model.generate(**inputs)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(pred.cpu()[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 蒙古国的首都是乌兰巴托（Ulaanbaatar）\n冰岛的首都是雷克雅未克（Reykjavik）\n埃塞俄比亚的首都是亚的斯亚贝巴（Addis Ababa）...</span></span><br></pre></td></tr></table></figure>

<h5 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h5><blockquote>
<p>注：作为术语的“tokenization”在中文中尚无共识的概念对应，本文档采用英文表达以利说明。</p>
</blockquote>
<p>基于tiktoken的tokenizer有别于其他分词器，比如sentencepiece tokenizer。尤其在微调阶段，需要特别注意特殊token的使用。关于tokenizer的更多信息，以及微调时涉及的相关使用，请参阅<a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen-7B/blob/main/tokenization_note_zh.md">文档</a>。</p>
<p>Qwen-7B采用**<u>UTF-8字节级别的BPE tokenization方式</u>**，并依赖<code>tiktoken</code>这一高效的软件包执行分词。 Qwen-7B中有两类token，即源于BPE、<code>bytes</code>类型的普通token和特殊指定、<code>str</code>类型的特殊token。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;Qwen/Qwen-7B&#x27;</span>, trust_remote_code=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h6 id="普通token"><a href="#普通token" class="headerlink" title="普通token"></a>普通token</h6><p>普通token源于BPE，是在UTF-8编码的文本字节序列上学习得到的。 尽管基于字节序列的方式保证了所有文本均可被tokenize且没有未登录token问题，但处理罕见文本时有可能回退到字节级别的编码。 由于从字节序列解码为文本时，<code>errors</code>参数设为<code>replace</code>，处理不完整的token序列可能会遇到UTF-8解码错误，表象是生成中包含“替换字符”(�)。 **<u>这一行为可以通过将<code>errors</code>参数设为<code>ignore</code>来规避</u>**。 一次性修改可以传入tokenizer的<code>decode</code>函数，持久性修改可以传入tokenizer的初始化函数，请注意<code>decode</code>的配置优先级更高。 <code>errors</code>的可选值，请参阅<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#bytes.decode">Python文档</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.decode([<span class="number">51461</span>])</span><br><span class="line"><span class="string">&#x27; �&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.convert_ids_to_tokens([<span class="number">51461</span>])</span><br><span class="line">[<span class="string">b&#x27; \xe6\xa0&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27; \xe6\xa0&#x27;</span>.decode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&#x27;replace&#x27;</span>)</span><br><span class="line"><span class="string">&#x27; �&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.decode([<span class="number">51461</span>, <span class="number">117</span>])</span><br><span class="line"><span class="string">&#x27; 根&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.convert_ids_to_tokens([<span class="number">51461</span>, <span class="number">117</span>])</span><br><span class="line">[<span class="string">b&#x27; \xe6\xa0&#x27;</span>, <span class="string">b&#x27;\xb9&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">b&#x27; \xe6\xa0\xb9&#x27;</span>.decode(<span class="string">&quot;utf-8&quot;</span>, errors=<span class="string">&#x27;replace&#x27;</span>)</span><br><span class="line"><span class="string">&#x27; 根&#x27;</span></span><br></pre></td></tr></table></figure>

<p><code>bytes</code>类型的普通token到id的映射可以通过<code>tokenizer.get_vocab()</code>获取。 尚不支持也不推荐向tokenizer增加普通token。</p>
<h6 id="特殊token"><a href="#特殊token" class="headerlink" title="特殊token"></a>特殊token</h6><p>特殊token用以给模型传递特殊信号，如到达文本末尾。 理论上，输入文本中不包含特殊token，它们仅在tokenization后由开发者手动加入。 特殊token的字面表达，如表示文本结束的<code>&lt;|endoftext|&gt;</code>，仅便于指代特殊token，不意味着它们在输入文本空间中。 目前，训练中使用的、已经有固定含义的、不应做它用的特殊token，Qwen-7B中有<code>&lt;|endoftext|&gt;</code>，Qwen-7B-Chat中有<code>&lt;|endoftext|&gt;</code>、<code>&lt;|im_start|&gt;</code>以及<code>&lt;|im_end|&gt;</code>。 但词表中也留有供扩展的特殊token位，可用<code>&lt;|extra_0|&gt;</code>到<code>&lt;|extra_204|&gt;</code>来指代。 <code>str</code>类型的特殊token字面表达到id的映射，可以通过<code>tokenizer.special_tokens</code>获取。</p>
<p>对于提供的模型参数(Qwen-7B和Qwen-7B-Chat)而言，诸如<code>bos</code>、<code>eos</code>、<code>unk</code>、<code>pad</code>、<code>mask</code>、<code>sep</code>等的特殊token的概念并不适用。 特例是<code>pad</code>，由于这个token理论上并不参与模型计算，所以可以使用任意token表达这一概念。 但保险起见，目前可在tokenizer初始化时设定的特殊token，仅可使用已知的特殊token字面表达，即<code>&lt;|endoftext|&gt;</code>、<code>&lt;|im_start|&gt;</code>、<code>&lt;|im_end|&gt;</code>和<code>&lt;|extra_0|&gt;</code>到<code>&lt;|extra_204|&gt;</code>。 对于微调或者其它需要这些token才能运行的框架，可以如下配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;Qwen/Qwen-7B&#x27;</span>, trust_remote_code=<span class="literal">True</span>, pad_token=<span class="string">&#x27;&lt;|endoftext|&gt;&#x27;</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意: 对于提供的训练好的模型，设置诸如<code>bos</code>、<code>eos</code>、<code>unk</code>之类的没有意义，即模型不需要这些概念。 如果设置了这些token，但没有相应的微调这些token以让模型理解其含义，未知行为可能被触发。 特别时，不应混淆<code>&lt;|endoftext|&gt;</code>和<code>eos</code>的概念，除非应用场景中它们的实际含义是一致的，即句子末尾等价于文本末尾。</p>
</blockquote>
<p><strong>注入攻击防御</strong></p>
<p>由于特殊token和普通token概念上的差异，如果输入文本中含有特殊token的字面表达该如何处理？ 以下面文本为例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>其正确的tokenization为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ids:[<span class="number">1350</span>, <span class="number">9639</span>, <span class="number">91</span>, <span class="number">8691</span>, <span class="number">723</span>, <span class="number">427</span>, <span class="number">91</span>, <span class="number">82598</span>]</span><br><span class="line">tokens: [<span class="string">b&#x27;print&#x27;</span>, <span class="string">b&#x27;(&quot;&lt;&#x27;</span>, <span class="string">b&#x27;|&#x27;</span>, <span class="string">b&#x27;endo&#x27;</span>, <span class="string">b&#x27;ft&#x27;</span>, <span class="string">b&#x27;ext&#x27;</span>, <span class="string">b&#x27;|&#x27;</span>, <span class="string">b&#x27;&gt;&quot;)&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>不是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ids: [<span class="number">1350</span>, <span class="number">445</span>, <span class="number">151643</span>, <span class="number">899</span>]</span><br><span class="line">tokens: [<span class="string">b&#x27;print&#x27;</span>, <span class="string">b&#x27;(&quot;&#x27;</span>, <span class="string">&#x27;&lt;|endoftext|&gt;&#x27;</span>, <span class="string">b&#x27;&quot;)&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>默认行为曾是正确的，即输入文本中任何字符一律按普通token处理，特殊token应由开发者在tokenization人工处理。 然后，这与社区中的实践似有差异，为开发者复用代码增加了额外适配步骤。</p>
<p>默认行为已被调整为从输入文本中解析特殊token的字面表达。 如需启用注入攻击防御，请传入参数<code>allowed_special=set()</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer(<span class="string">&#x27;print(&quot;&lt;|endoftext|&gt;&quot;)&#x27;</span>, allowed_special=<span class="built_in">set</span>())</span><br><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">1350</span>, <span class="number">9639</span>, <span class="number">91</span>, <span class="number">8691</span>, <span class="number">723</span>, <span class="number">427</span>, <span class="number">91</span>, <span class="number">82598</span>], <span class="string">&#x27;token_type_ids&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">&#x27;attention_mask&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure>

<p>这一行为可以更精细的调控，将<code>allowed_special</code>设计为<code>str</code>的集合即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer(<span class="string">&#x27;print(&quot;&lt;|extra_0|&gt;&quot;)&lt;|endoftext|&gt;&#x27;</span>, allowed_special=&#123;<span class="string">&#x27;&lt;|endoftext|&gt;&#x27;</span>&#125;)</span><br><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">1350</span>, <span class="number">9639</span>, <span class="number">91</span>, <span class="number">15460</span>, <span class="number">62</span>, <span class="number">15</span>, <span class="number">91</span>, <span class="number">82598</span>, <span class="number">151643</span>], <span class="string">&#x27;token_type_ids&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">&#x27;attention_mask&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure>

<p>如果希望输入中遇到特殊token的字面表达时，获得更直接的提醒，通过配置<code>disallowed_special</code>可以让tokenizer直接触发异常：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer(<span class="string">&#x27;print(&quot;&lt;|extra_0|&gt;&quot;)&lt;|endoftext|&gt;&#x27;</span>, allowed_special=&#123;<span class="string">&#x27;&lt;|endoftext|&gt;&#x27;</span>&#125;, disallowed_special=(<span class="string">&#x27;&lt;|extra_0|&gt;&#x27;</span>, ))</span><br><span class="line">...</span><br><span class="line">ValueError: Encountered text corresponding to disallowed special token <span class="string">&#x27;&lt;|extra_0|&gt;&#x27;</span>.</span><br><span class="line">If you want this text to be encoded <span class="keyword">as</span> a special token, <span class="keyword">pass</span> it to `allowed_special`, e.g. `allowed_special=&#123;<span class="string">&#x27;&lt;|extra_0|&gt;&#x27;</span>, ...&#125;`.</span><br><span class="line">If you want this text to be encoded <span class="keyword">as</span> normal text, disable the check <span class="keyword">for</span> this token by passing `disallowed_special=(enc.special_tokens_set - &#123;<span class="string">&#x27;&lt;|extra_0|&gt;&#x27;</span>&#125;)`.</span><br><span class="line">To disable this check <span class="keyword">for</span> <span class="built_in">all</span> special tokens, <span class="keyword">pass</span> `disallowed_special=()`.</span><br></pre></td></tr></table></figure>

<p>更多关于<code>allowed_special</code>和<code>disallowed_special</code>的信息, 请参阅<a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken/blob/095924e02c85617df6889698d94515f91666c7ea/tiktoken/core.py#L75"><code>tiktoken</code>代码</a>.</p>
<p>新的默认行为与以下设定等价</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer(<span class="string">&#x27;print(&quot;&lt;|endoftext|&gt;&quot;)&#x27;</span>, allowed_special=<span class="string">&quot;all&quot;</span>, disallowed_special=())</span><br><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">1350</span>, <span class="number">445</span>, <span class="number">151643</span>, <span class="number">899</span>], <span class="string">&#x27;token_type_ids&#x27;</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">&#x27;attention_mask&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure>

<h5 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h5><p>如希望使用更低精度的量化模型，如4比特和8比特的模型，我们提供了简单的示例来说明如何快速使用量化模型。在开始前，确保你已经安装了<code>bitsandbytes</code>。请注意，<code>bitsandbytes</code>的安装要求是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">**Requirements** Python &gt;=3.8. Linux distribution (Ubuntu, MacOS, etc.) + CUDA &gt; 10.0.</span><br></pre></td></tr></table></figure>

<p>Windows用户需安装特定版本的<code>bitsandbytes</code>，可选项包括<a target="_blank" rel="noopener" href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels">bitsandbytes-windows-webui</a>。</p>
<p>你只需要在<code>AutoModelForCausalLM.from_pretrained</code>中添加你的量化配置，即可使用量化模型。如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, BitsAndBytesConfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># quantization configuration for NF4 (4 bits)</span></span><br><span class="line">quantization_config = BitsAndBytesConfig(</span><br><span class="line">    load_in_4bit=<span class="literal">True</span>,</span><br><span class="line">    bnb_4bit_quant_type=<span class="string">&#x27;nf4&#x27;</span>,</span><br><span class="line">    bnb_4bit_compute_dtype=torch.bfloat16</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># quantization configuration for Int8 (8 bits)</span></span><br><span class="line">quantization_config = BitsAndBytesConfig(load_in_8bit=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    args.checkpoint_path,</span><br><span class="line">    device_map=<span class="string">&quot;cuda:0&quot;</span>,</span><br><span class="line">    quantization_config=quantization_config,</span><br><span class="line">    max_memory=max_memory,</span><br><span class="line">    trust_remote_code=<span class="literal">True</span>,</span><br><span class="line">).<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>

<p>上述方法可以让我们将模型量化成<code>NF4</code>和<code>Int8</code>精度的模型进行读取，帮助我们节省显存开销。我们也提供了相关性能数据。我们发现尽管模型在效果上存在损失，但模型的显存开销大幅降低。</p>
<table>
<thead>
<tr>
<th>Precision</th>
<th>MMLU</th>
<th>Memory</th>
</tr>
</thead>
<tbody><tr>
<td>BF16</td>
<td>56.7</td>
<td>16.2G</td>
</tr>
<tr>
<td>Int8</td>
<td>52.8</td>
<td>10.1G</td>
</tr>
<tr>
<td>NF4</td>
<td>48.9</td>
<td>7.4G</td>
</tr>
</tbody></table>
<h5 id="工具调用"><a href="#工具调用" class="headerlink" title="工具调用"></a>工具调用</h5><p>Qwen-7B-Chat针对包括API、数据库、模型等工具在内的调用进行了优化。用户可以开发基于Qwen-7B的LangChain、Agent甚至Code Interpreter。我们在内部的即将开源的评测数据集上测试模型的工具调用能力，并发现Qwen-7B-Chat能够取得稳定的表现。</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Tool Selection (Acc.↑)</th>
<th>Tool Input (Rouge-L↑)</th>
<th>False Positive Error↓</th>
</tr>
</thead>
<tbody><tr>
<td>GPT-4</td>
<td>95%</td>
<td><strong>0.90</strong></td>
<td>15%</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>85%</td>
<td>0.88</td>
<td>75%</td>
</tr>
<tr>
<td><strong>Qwen-7B</strong></td>
<td><strong>99%</strong></td>
<td>0.89</td>
<td><strong>8.5%</strong></td>
</tr>
</tbody></table>
<p>我们提供了文档说明如何根据ReAct Prompting的原则写作你的prompt。</p>
<p>For how to write and use prompts for ReAct Prompting, please refer to <a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen-7B/blob/main/examples/react_prompt.md">the ReAct examples</a>。</p>
<p>此外，我们还提供了实验结果表明我们的模型扮演Agent的能力。请阅读相关文档<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/transformers_agents">链接</a>了解更多信息。模型在Hugging Face提供的评测数据集上表现如下：</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Tool Selection↑</th>
<th>Tool Used↑</th>
<th>Code↑</th>
</tr>
</thead>
<tbody><tr>
<td>GPT-4</td>
<td><strong>100</strong></td>
<td><strong>100</strong></td>
<td><strong>97.41</strong></td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>95.37</td>
<td>96.30</td>
<td>87.04</td>
</tr>
<tr>
<td>StarCoder-15.5B</td>
<td>87.04</td>
<td>87.96</td>
<td>68.89</td>
</tr>
<tr>
<td><strong>Qwen-7B</strong></td>
<td>90.74</td>
<td>92.59</td>
<td>74.07</td>
</tr>
</tbody></table>
<h5 id="长文本理解"><a href="#长文本理解" class="headerlink" title="长文本理解"></a>长文本理解</h5><p>我们引入了**<u><em>NTK插值、窗口注意力、LogN注意力缩放</em></u>**等技术来提升模型的上下文长度并突破训练序列长度的限制。我们的模型已经突破8K的序列长度。通过arXiv数据集上的语言模型实验（PPL），我们发现Qwen-7B能够在长序列的设置下取得不错的表现。</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Sequence Length</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>1024</td>
<td>2048</td>
<td>4096</td>
<td>8192</td>
<td>16384</td>
<td></td>
</tr>
<tr>
<td>Qwen-7B</td>
<td><strong>4.23</strong></td>
<td><strong>3.78</strong></td>
<td>39.35</td>
<td>469.81</td>
<td>2645.09</td>
</tr>
<tr>
<td>+ dynamic_ntk</td>
<td><strong>4.23</strong></td>
<td><strong>3.78</strong></td>
<td>3.59</td>
<td>3.66</td>
<td>5.71</td>
</tr>
<tr>
<td>+ dynamic_ntk + logn</td>
<td><strong>4.23</strong></td>
<td><strong>3.78</strong></td>
<td><strong>3.58</strong></td>
<td>3.56</td>
<td>4.62</td>
</tr>
<tr>
<td>+ dynamic_ntk + logn + local_attn</td>
<td><strong>4.23</strong></td>
<td><strong>3.78</strong></td>
<td><strong>3.58</strong></td>
<td><strong>3.49</strong></td>
<td><strong>4.32</strong></td>
</tr>
</tbody></table>
<h4 id="QWen微调方案"><a href="#QWen微调方案" class="headerlink" title="QWen微调方案"></a>QWen微调方案</h4><p>【1】ModelScope 版本 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/z5JNbKOjYV6vkqK6KyrSQg">https://mp.weixin.qq.com/s/z5JNbKOjYV6vkqK6KyrSQg</a></p>
<ol>
<li><strong>环境准备</strong></li>
</ol>
<p><strong>硬件配置建议</strong></p>
<ul>
<li>处理器：推荐使用第 12 代 Intel Core i7 或更高配置</li>
<li>内存：建议 48GB 以上（尽管 16GB 也可以，但加载模型速度较慢）</li>
<li>显卡：推荐使用 NVIDIA GeForce RTX 3080 或更高配置，显存 24GB 以上。（由于目前框架似乎还没有看到推理加速，所以，24GB或者以上是需要的。）</li>
</ul>
<p><strong>部署环境</strong></p>
<p>使用 nvidia-docker 部署最新的 pytorch以及modelscope，切记不要使用真实环境，一旦出现包存在问题，导致回滚会非常浪费时间。</p>
<p>依次检验：nvidia-smi,pytorch,以及modelscope </p>
<p>查看环境是否部署好：nvidia-smi</p>
<p>查看显卡是否能够正常识别 : pytorch</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.get_device_name(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>查看modelscope的版本 是否&gt;1.8.1 : pip list | grep modelscope </p>
<p><strong>2. 模型准备</strong></p>
<ol>
<li><strong>下载模型 ，执行如下代码，会自动下载, 切记模型的ID是</strong> Qwen&#x2F;Qwen-7b 【下载完后会在 &#x2F;root&#x2F;.cache&#x2F;modelscope&#x2F;hub&#x2F;Qwen&#x2F;Qwen-7b 看到模型】</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from modelscope.hub.snapshot_download import snapshot_download</span><br><span class="line">model_dir = snapshot_download(&#x27;Qwen/Qwen-7b&#x27;, &#x27;v1.0.0&#x27;)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p><strong>下载训练脚本</strong></p>
<p>下载 <a target="_blank" rel="noopener" href="https://github.com/modelscope/swift">https://github.com/modelscope/swift</a></p>
<p>找到 examples&#x2F;pytorch&#x2F;llm&#x2F; , 里面的run_sft.sh 是训练文件，run_infer.sh 是推理文件</p>
</li>
<li><p><strong>预加载训练数据</strong></p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 \</span><br><span class="line">python llm_sft.py \</span><br><span class="line">    --model_type qwen-7b \</span><br><span class="line">    --sft_type lora \</span><br><span class="line">    --output_dir runs \</span><br><span class="line">    --dataset alpaca-en,alpaca-zh \</span><br><span class="line">    --dataset_sample 20000 \</span><br><span class="line">    --max_length 1024 \</span><br><span class="line">    --quantization_bit 4 \</span><br><span class="line">    --lora_rank 8 \</span><br><span class="line">    --lora_alpha 32 \</span><br><span class="line">    --lora_dropout_p 0.1 \</span><br><span class="line">    --batch_size 1 \</span><br><span class="line">    --learning_rate 1e-4 \</span><br><span class="line">    --gradient_accumulation_steps 16 \</span><br><span class="line">    --eval_steps 100 \</span><br><span class="line">    --save_steps 100 \</span><br><span class="line">    --save_total_limit 2 \</span><br><span class="line">    --logging_steps 10 \</span><br></pre></td></tr></table></figure>

<p>看了一下代码，好像修改数据的目录有点麻烦，也不知道数据格式咋样，先直接执行 PYTHONPATH&#x3D;..&#x2F;..&#x2F;.. bash run_sft.sh ，预先加载数据看一下，看到有datasets的文件目录出来后可以kill掉</p>
<p>可以看到 &#x2F;root&#x2F;.cache&#x2F;modelscope&#x2F;hub&#x2F;datasets&#x2F;AI-ModelScope&#x2F;  保存好了训练的数据看一下格式，是由 instruction,input ,output三个字段的组成的csv，那我们只要按照这个格式写入改数据即可把微调替换成我们的数据。</p>
<p>至此，模型的准备基本完成。</p>
<ol start="3">
<li><strong>训练数据准备</strong></li>
</ol>
<p>本次的微调依旧是改变大模型的自我认知。直接拿起旧的数据跑，把该数据转换成CSV替换掉&#x2F;root&#x2F;.cache&#x2F;modelscope&#x2F;hub&#x2F;datasets&#x2F;AI-ModelScope&#x2F;的alpaca-gpt4-data-zh</p>
<p>修改训练脚本，只加载单个数据集，增加多轮的训练次数。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 \</span><br><span class="line">python llm_sft.py \</span><br><span class="line">    --model_type qwen-7b \</span><br><span class="line">    --sft_type lora \</span><br><span class="line">    --output_dir runs \</span><br><span class="line">    --dataset alpaca-zh \</span><br><span class="line">    --dataset_sample 20 \</span><br><span class="line">    --max_length 1024 \</span><br><span class="line">    --quantization_bit 4 \</span><br><span class="line">    --lora_rank 8 \</span><br><span class="line">    --lora_alpha 32 \</span><br><span class="line">    --lora_dropout_p 0.1 \</span><br><span class="line">    --batch_size 1 \</span><br><span class="line">    --learning_rate 1e-4 \</span><br><span class="line">    --gradient_accumulation_steps 16 \</span><br><span class="line">    --eval_steps 100 \</span><br><span class="line">    --save_steps 100 \</span><br><span class="line">    --save_total_limit 2 \</span><br><span class="line">    --logging_steps 10 \</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注：</p>
<ol>
<li>程序会按照加载模型－＞ 加载训练数据－＞ 训练－＞ 保存到 output_dir 4 个步骤进行。可以查看自己在哪一个步骤出错，进行修改。</li>
<li>每个人的环境不一样，训练的时间不一样，我这个配置训练的时间是大概 10 几分钟就走完。</li>
</ol>
</blockquote>
<ol start="4">
<li><strong>训练结果</strong></li>
</ol>
<p>十几分钟后，在 run文件夹下看到看到训练好的模型， 一般取最新的。</p>
<p>修改run_infer.sh 把ckpt_dir只向我们刚训练完的模型</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 \</span><br><span class="line">python llm_infer.py \</span><br><span class="line">    --model_type qwen-7b \</span><br><span class="line">    --ckpt_dir runs/qwen-7b\v1-20230807-002814\checkpoint-100 \</span><br><span class="line">    --eval_human true \</span><br><span class="line">    --dataset_sample 19 \</span><br><span class="line">    --max_new_tokens 1024 \</span><br><span class="line">    --temperature 0.9 \</span><br><span class="line">    --top_k 50 \</span><br><span class="line">    --top_p 0.9 \</span><br><span class="line">    --do_sample true \</span><br></pre></td></tr></table></figure>

<p>执行如下代码 PYTHONPATH&#x3D;..&#x2F;..&#x2F;.. bash run_infer.sh ，添加微调结果，启动模型，可以看到，大模型的认知已经改变。</p>
<ol start="5">
<li><strong>一些问题和想法</strong></li>
</ol>
<blockquote>
<p><strong>1. 当前swift 还在迭代中，可能存在不稳定的情况。</strong></p>
<p>例如5号的代码能正常在3090跑，最近更新的一个版本似乎不行了，我又会退了一下。</p>
<p>例如在5号的时候，还不支持量化，7号的时候已经可以了。 大家如果执行不稳定的话，可以先固化一下可以跑的版本慢慢等优化。</p>
<p><strong>2.  微调的效果很好，没有出现知识遗忘的情况。</strong></p>
<p><strong>3.  由于框架和llama类似，期待后续的加速方案以及生态工具加上。</strong></p>
</blockquote>
<p>仓库链接：<a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md#introducing-qwen-7b-open-foundation-and-human-aligned-models-of-the-state-of-the-arts">https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md#introducing-qwen-7b-open-foundation-and-human-aligned-models-of-the-state-of-the-arts</a></p>
<h3 id="InternLM"><a href="#InternLM" class="headerlink" title="InternLM"></a>InternLM</h3><h3 id="Baichuan"><a href="#Baichuan" class="headerlink" title="Baichuan"></a>Baichuan</h3><h4 id="latest-news"><a href="#latest-news" class="headerlink" title="latest news"></a><strong>latest news</strong></h4><p>百川智能4月10日成立后，6月15日发布了70亿参数规模开源模型Baichuan-7B，7月11日发布了130亿参数规模大模型Baichuan-13B。</p>
<p>2023年8月8日下午，百川智能发布530亿参数规模的闭源大模型Baichuan-53B，这是百川智能发布的第三个大模型，主要服务B端行业，预计下个月将会开放API等相关组件。【这次大模型的文科能力更好，比如在理解古诗、生成有个性化风格的文章等方面。】</p>
<p>融合了意图理解、信息检索以及强化学习技术，在知识问答、文本创作领域表现突出</p>
<blockquote>
<p>百川智能技术联合创始人陈炜鹏：</p>
<p>首先，做大模型的第一个环节是数据从哪来，中文互联网网页中的数据高达万亿、百亿量级，搜狗此前的数据积累，能让他们知道哪里有好的数据，并且将这些数据进行收集、处理、识别，在这一领域，百川智能目前的团队有很强的技术积累和方法论。</p>
<p>在英文数据方面也是如此，他补充道，搜狗在翻译领域的积累也有很多。</p>
<p>其次，模型本身的训练，模型的训练是一个相对复杂的系统，陈炜鹏谈道，这包括**<u>数据的获取、选择、配比、标注，数据准备好之后模型的训练框架</u><strong>，</strong><u>网络的运营效率如何组成框架，不同的算法如何组合，选用什么样的网络结构统领这些，如何评价这个事情，算法的选择等</u>**。百川智能此前推出的70亿参数规模大模型在并行策略方面做的比较好，有技术积累。</p>
<p>最后，百川智能目前的技术团队有很多来自字节跳动、百度、华为的技术人才，也使得其技术能力更加多元。</p>
<p>综上，在技术和人才的共同加持下，百川智能在大模型的研发方面走的比较快。</p>
</blockquote>
<h4 id="Baichuan-7B"><a href="#Baichuan-7B" class="headerlink" title="Baichuan-7B"></a>Baichuan-7B</h4><h4 id="Baichuan-53B"><a href="#Baichuan-53B" class="headerlink" title="Baichuan-53B"></a>Baichuan-53B</h4><p><strong>预训练数据（特点）：</strong></p>
<p>–全面的世界知识体系</p>
<p>– 系统的数据质量体系</p>
<p>–多粒度的大规模聚类系统</p>
<p>–细粒度自动化匹配算法</p>
<p>**<u>搜索增强是解决模型时效性和幻觉的有效手段</u>**，因此，百川智能将搜索技术与大语言模型能力相结合，实现创新性的模型优化与改进。</p>
<p>搜索增强系统**<u>融合了指令意图理解、智能搜索和结果增强等关键组件</u>**，这一综合体系通过深入理解用户指令，精确驱动查询词的搜索，并结合大语言模型技术来优化模型结果生成的可靠性，基于此，百川智能实现了更精确、更智能的模型结果回答，减少了模型的幻觉。</p>
<p>其中，**<u>动态响应策略方面，百川智能将指令任务细化为16个独立类别，涵盖了用户指令的精准问答、逻辑推理、头脑风暴等各种场景，并针对每一个指令类别都进行了设计和优化</u>**。</p>
<p>**<u>智能化搜索词生成</u>**则是通过对问答样本进行精细化人工标注，捕捉和理解用户多元化的指令需求，大模型负责执行一系列关键任务，如时效性识别和搜索意图判别，从而准确解释用户的查询意图并精准响应。</p>
<p>为了达到高质量搜索结果筛选，百川智能构建了一个**<u>搜索结果相关性模型</u>**，对从搜索内容和知识库中获取的信息进行相关性评分。</p>
<p>在回答结果的搜索增强上，百川智能**<u>采用RLHF（人类反馈强化学习）技术，使得大模型能够参照搜索结果，针对用户请求生成高价值且具有实时性的回答</u>**。</p>
<p>除此以外，大模型还会通过对齐调整让模型同人类价值观对齐，生成令人满意的回复内容。</p>
<h2 id="多语言大模型"><a href="#多语言大模型" class="headerlink" title="多语言大模型"></a>多语言大模型</h2><h3 id="LLaMa"><a href="#LLaMa" class="headerlink" title="LLaMa"></a>LLaMa</h3><h4 id="LLaMA-2"><a href="#LLaMA-2" class="headerlink" title="LLaMA-2"></a>LLaMA-2</h4><p>文章地址<a target="_blank" rel="noopener" href="https://together.ai/blog/llama-2-7b-32k%E4%B8%AD%E6%8A%A5%E9%81%93%E4%BA%86LLaMA-2-7B-32K%E8%BF%99%E4%B8%80%E5%B7%A5%E4%BD%9C%EF%BC%8C%E8%AF%A5%E6%A8%A1%E5%9E%8B%E5%B0%86LLaMA-2-7B%E6%89%A9%E5%B1%95%E5%88%B032K%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%EF%BC%8C%E4%BD%BF%E7%94%A8Meta%E7%9A%84[%E6%8F%92%E5%80%BC](https://arxiv.org/abs/2306.15595)%E3%80%81%E7%BB%A7%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83%E3%80%81FlashAttention%EF%BC%8C%E8%83%BD%E5%A4%9F%E5%A4%84%E7%90%86%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BB%BB%E5%8A%A1%EF%BC%88%E5%A6%82%E5%A4%9A%E6%96%87%E6%A1%A3%E7%90%86%E8%A7%A3%E3%80%81%E6%91%98%E8%A6%81%E5%92%8C">https://together.ai/blog/llama-2-7b-32k中报道了LLaMA-2-7B-32K这一工作，该模型将LLaMA-2-7B扩展到32K长上下文，使用Meta的[插值](https://arxiv.org/abs/2306.15595)、继续预训练、FlashAttention，能够处理长上下文任务（如多文档理解、摘要和</a> QA)。</p>
<h5 id="长文本扩展"><a href="#长文本扩展" class="headerlink" title="长文本扩展"></a>长文本扩展</h5><p>长上下文模型对于文档理解、摘要和检索增强生成已经至关重要。</p>
<p>将 LLaMA-2 (4K) 扩展到 32K 上下文是如何做到的，我们可以看看其实现逻辑。LLaMA-2的上下文长度为4Ktoken。要将其扩展到32K上下文，该工作分成了三个部分：<strong>建模、数据和系统优化(包括<a target="_blank" rel="noopener" href="https://together.ai/blog/tri-dao-flash-attention">Flash-Attention-2</a>)。</strong></p>
<p>【1】建模</p>
<p>首先，在建模方面，我**<u>使用线性插值来扩展上下文长度</u>**，线性插值法提供了一种有效的方法来扩展具有旋转位置嵌入的模型的上下文长度。使用LLaMA-2检查点，并继续使用1.5B token的线性插值对其进行预训练&#x2F;微调。</p>
<p>【2】数据</p>
<p>其次，**<u>在数据方面</u><strong>，不是简单地使用Pile和RedPajama等通用语言数据集进行微调，主要考虑到2点，一个是</strong><u>需要模型的通用长上下文语言数据来学习如何处理插值位置嵌入，另一个是需要指令数据来鼓励模型在长上下文中实际利用信息</u>**。</p>
<blockquote>
<p>所以，采用了<strong>数据混合</strong>的方式:</p>
<p>[1] <strong>在继续预训练阶段</strong>，数据构成上包含25%的RedPajama Book、25% 的RedPajama ArXiv（包括摘要）、25%来自RedPajama的其他数据，以及 25% 来自 UL2 Oscar Data(这是 OIG(<a target="_blank" rel="noopener" href="https://github.com/LAION-AI/Open-Instruction-Generalist">Open-Instruction-Generalist</a>) 的一部分)，要求模型填写缺失的块或完成文本。为了增强长上下文功能，剔除了短于2K token的序列。UL2 Oscar数据鼓励模型对长程依赖进行建模。</p>
<p>[2] <strong>在微调阶段</strong>，专注于其具有长上下文的few shot能力，包括20%自然指令（NI）、20%的Public Pool of Prompts（P3）、20%的Pile。为了减轻遗忘，进一步将20%的RedPajama Book和20%的RedPajama ArXiv与摘要相结合。</p>
</blockquote>
<p>最后，<strong>在评估阶段</strong>，针对HELM核心场景(HELM core scenarios) <a target="_blank" rel="noopener" href="https://together.ai/blog/redpajama-7b">here</a>进行了数据去重，并做细致评估，具体包括：（1）在PG-19上不同序列长度下的归一化困惑度，（2）以及在16个核心场景中的HELM v1.0得分（在适合LLaMA 2的相同上下文长度上评估），可以看到，LLaMA-2-7B-32K产生合理的困惑，与原始LLaMA 2模型相当。此外，在HELM v1.0上，LLaMA-2-7B-32K与原始LLaMA-2-7B base相比，即使没有达到更好质量，但也是相当。Perplexity-per-byte for various context lengths:<br>$$<br>exp(1&#x2F;N_{byte} sum_{i&#x3D;1,…,N_{tokens}} loss_i)<br>$$<br><strong>微调构建长上下文应用场景数据形式</strong></p>
<p>LLaMA-2-7B-32K 的强大之处在于它形成了一个强大的基座模型，人们可以对其进行微调以构建自己的应用程序。我们现在举例说明两个这样的例子。</p>
<p><strong>（1）Long-context QA.</strong></p>
<p>我们以论文“迷失在中间：语言模型如何使用长上下文”中的多文档问答任务为例。模型的输入包括 1） 需要答案的问题和 2） k 个文档，这些文档是从维基百科中提取的段落。值得注意的是，这些文档中只有一个文档包含问题的答案，而其余的k − 1文档（称为“干扰”文档）则不包含。若要成功执行此任务，模型必须从其输入上下文中识别并利用包含答案的文档。一个潜在的用例是实现LLM与文档和vector database之间的无缝集成，vector database用于获取相关信息（上下文），LLM用于回答用户的问题。</p>
<blockquote>
<p>微调在长上下文 QA 中表现更好的模型，按以下格式准备数据：</p>
<p><em>&#96;&#96;&#96;<br>Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant).</em></p>
<p><em>Document [1] (Title: Email retargeting) on sending personalized e-mail to an anonymous website visitor…</em></p>
<p><em>Document [2] (Title: Opt-in email) of 2003 does not require an opt-in approach, only an easy opt-out system…</em></p>
<p><em>Document [3] (Title: Email marketing) to send direct promotional messages to, or they rent a list of email addresses …</em></p>
<p><em>…</em></p>
<p><em>Question: which is the most common use of opt-in e-mail marketing</em></p>
<p><em>Answer: a newsletter sent to an advertising firm’s customers</em></p>
<p><em>&#96;&#96;&#96;</em></p>
<p>我们的预处理过程反映了上述论文中使用的过程，并且我们从NaturalQuestion数据集中得出了我们的训练集。training&#x2F;finetune_LLaMA-2-7b-32k-mqa.sh 说明了如何将此数据集传递给 OCK 以微调 LLaMA-2-7B-32K。</p>
<p>我们通过在上下文中打包的不同数量(从 20 到 100)的文档来衡量质量。平均而言，这相当于模型输入中的 2.9K 个token到 14.8K 个token。正如我们所看到的，一旦我们在这项任务上微调LLaMA-2-7B-32K，我们就实现了质量的显着提高。</p>
</blockquote>
<p><strong>（2）Long-context summarization.</strong> </p>
<p>我们使用BookSum，这是一个独特的数据集，旨在解决长篇叙事摘要的挑战。该数据集包含文学领域的源文档，包括小说、戏剧和故事，并提供人类编写的高度抽象摘要。我们在这里关注章节级别的数据。 BookSum 提出了一系列独特的挑战，需要模型全面通读每一章。</p>
<blockquote>
<p>我们按以下格式准备数据：</p>
<p><em>&#96;&#96;&#96;</em></p>
<p><em>Chapter: “Mother, Mother, I am so happy!” whispered the girl, burying her face in the lap of the faded, tired-looking woman who, with back turned to the shrill intrusive light, was sitting in the one arm-chair that their dingy sitting-room contained. “I am so happy!” she repeated, “and you must be happy, too!”…</em></p>
<p><em>Q: Can you write an appropriate summary of the above paragraphs?</em></p>
<p><em>A: The following day, Sibyl Vane and her mother discuss the girl’s relationship with “Prince Charming.” Sibyl is elated and wants her mother to share her joy. She is in love. Mrs. Vane’s attitude is more realistic and down-to-earth. She wants her daughter to think of her career…</em></p>
<p><em>&#96;&#96;&#96;</em></p>
<p>我们可以通过训练&#x2F;finetune_LLaMA-2-7b-32k-booksum.sh 在这个数据集上微调LLaMA-2-7B-32K。</p>
<p>测试数据平均包含大约 4500 个token。为了评估和比较性能，我们计算了三个特定指标：Rouge-1、Rouge-2 和 Rouge-L 分数。对于 LLaMA-2，当输入不适合 4K 上下文时，我们会截断输入。我们看到，微调后的模型在所有这些指标上都获得了更高的分数。</p>
</blockquote>
<p>【3】系统优化</p>
<p>构建长上下文模型的一个独特挑战是，不断增加的上下文长度需要系统优化。</p>
<p>我们发布了一个更新的训练和推理堆栈，集成了我们的首席科学家Tri Dao最近发布的FlashAttention-2，以及一系列其他优化：</p>
<blockquote>
<ul>
<li>当前的<a target="_blank" rel="noopener" href="https://github.com/togethercomputer/OpenChatKit">OCK repo</a>现在支持使用 32K 上下文进行微调。通过最新的优化，我们使用FlashAttention-1实现了比优化良好的OCK高出1.6倍。</li>
<li>我们还将 FlashAttention-2 集成到 <a target="_blank" rel="noopener" href="https://huggingface.co/togethercomputer/Llama-2-7B-32K-beta/">inference stack</a> 中，可以使用 HuggingFace Transformer 运行;在 32K 上下文中，与最先进的模型相比，它的推理吞吐量提高了 3 倍。</li>
</ul>
</blockquote>
<h3 id="BLOOM"><a href="#BLOOM" class="headerlink" title="BLOOM"></a>BLOOM</h3><h3 id="Falcon"><a href="#Falcon" class="headerlink" title="Falcon"></a>Falcon</h3><h1 id="领域LLM模型"><a href="#领域LLM模型" class="headerlink" title="领域LLM模型"></a>领域LLM模型</h1><h2 id="儿童陪伴"><a href="#儿童陪伴" class="headerlink" title="儿童陪伴"></a>儿童陪伴</h2><p><strong>【1】面向儿童心理健康领域的微调模型QiaoBan</strong></p>
<p>该项目旨在构建一个面向儿童情感陪伴的大模型，<strong>主要面向K12中小学生及家长群体，是一个7B规模的大语言模型，其通过给定话题下进行数据的生成，可以作为多轮对话的一个测试集使用，其场景也很有趣。</strong></p>
<p><em><strong>地址：<a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR-SC/QiaoBan">https://github.com/HIT-SCIR-SC/QiaoBan</a></strong></em></p>
<p>在训练数据上，从真实场景的儿童对话话题列表中进行采样，选定当前对话话题，在儿童情绪辅导理论的指导下，<strong>构建了1k余段高质量中文儿童情感陪伴对话数据</strong>。</p>
<p>数据构建过程由经过儿童情绪辅导理论培训的志愿者完成，同时邀请了儿童心理学领域的专家学者对数据的收集过程提出建议与指导，以确保数据的准确性和可信度。</p>
<p>从项目中的<strong>话题列表</strong>共538个（文件：<a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR-SC/QiaoBan/blob/main/topic.txt%EF%BC%89%E9%83%A8%E5%88%86%E5%A6%82%E4%B8%8B%EF%BC%9A">https://github.com/HIT-SCIR-SC/QiaoBan/blob/main/topic.txt）部分如下：</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">班会游戏、新年班会、评奖班会、班级聚餐、班级聚会</span><br><span class="line">元旦晚会、才艺晚会、儿童节文艺表演、“快乐童年”才艺小舞台、儿童节游园、春节联欢晚会、运动会、歌唱比赛、诗朗诵比赛、年级篮球赛</span><br><span class="line">动物园、博物馆、风景区、春秋游</span><br><span class="line">篮球、足球、跑步、羽毛球、乒乓球、台球、游泳</span><br><span class="line">拾金不昧、热爱劳动、诚实、乐于助人、自主为班级搬东西</span><br></pre></td></tr></table></figure>

<p>完整的数据可以查看：<a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR-SC/QiaoBan/blob/main/data/child_chat_data.json">https://github.com/HIT-SCIR-SC/QiaoBan/blob/main/data/child_chat_data.json</a></p>
<p>例如，给<strong>定话题生成</strong>的数据如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;dialog_id&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;emotion&quot;</span><span class="punctuation">:</span> <span class="string">&quot;angry&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;topic&quot;</span><span class="punctuation">:</span> <span class="string">&quot;班会游戏&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;dialog&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;妈妈，老师说我没赢游戏，但我真的赢了，我好生气，觉得太不公平了！&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;孩子&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;亲爱的，我了解你的感受，可以跟我说说班会上的情况吗？&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;智能助手&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我们班有个游戏，我得分最高，团队也赢了，但老师却说是别的同学赢了，这太不公平了！&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;孩子&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我明白了，但是你是怎么赢的游戏呢？&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;智能助手&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我得分最高，团队也赢了，所有人都看到我是赢家，为什么老师还说我没赢呢？&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;孩子&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;那你知道老师为什么这么说吗？&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;智能助手&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;不知道，但我觉得太不公平了！&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;孩子&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;是啊，很不公平，但是我们也要尊重老师的决定，也许他有自己的原因。你可以去跟老师聊一下，了解他的想法，这样你会更清楚事情的真相。如果你还是觉得很不公平，我们可以一起找班主任或校长谈谈，但是在处理问题时，我们要保持冷静，不要因为情绪而做出错误的决定。&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;智能助手&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;好的，我会去跟老师聊聊。谢谢你，妈妈。&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;孩子&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;没事，我永远支持你，如果需要帮助，随时来找我哦。&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;智能助手&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;妈妈，还有一件事我想跟你说。&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;孩子&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;什么事？你可以跟我说说看。&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;智能助手&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今天我跟同学吃午饭时，有个同学讲了很多关于环保的事情，让我感到很惊讶。&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;孩子&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;哦？那你觉得他讲了些什么有趣的事情吗？&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;智能助手&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;他说我们要多使用可重复利用的袋子，可以减少塑料袋的浪费，还说我们要多种植树木，因为树木可以吸收二氧化碳，减少空气污染。我觉得这些主意很好，如果每个人都能做到，地球就会更加美好了！&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;孩子&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;太好了！你的同学有很棒的想法，你也可以向他学习哦。你可以跟他多聊聊环保方面的知识，也可以尝试在家里做些环保的事情，比如垃圾分类、节约用水等等。&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;智能助手&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;好的，我会的！我觉得环保真的很重要，我们要尽力保护我们的地球。&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;孩子&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;是的，这是我们每个人都应该做的事情。我很高兴你能意识到这一点，继续加油哦！&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;speaker&quot;</span><span class="punctuation">:</span> <span class="string">&quot;智能助手&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>此外，根据该项目描述，其通过话题采样选定当前对话话题，结合儿童情绪辅导理论指导下的prompt，共同组成chatgpt_prompt，从gpt-3.5-turbo中获取<strong>5千段</strong>儿童情感陪伴对话数据。</p>
<p>在训练耗费上，<strong>在四张A100-80GB的GPU卡上进行指令微调</strong>，大约花费50个小时完成训练过程。</p>
<h1 id="Agent相关"><a href="#Agent相关" class="headerlink" title="Agent相关"></a>Agent相关</h1><h2 id="HuggingFace-Agent"><a href="#HuggingFace-Agent" class="headerlink" title="HuggingFace Agent"></a>HuggingFace Agent</h2><p>使用大模型作为Agent，仅需自然语言就可调用HuggingFace中的模型，目前支持两种模式：</p>
<ul>
<li>run模式：单轮对话，没有上下文，单个prompt多tool组合调用能力好</li>
<li>chat模式：多轮对话，有上下文，单次调用能力好，可能需要多次prompt实现多tool组合调用</li>
</ul>
<blockquote>
<p>详见官方文档：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/transformers_agents">Transformers Agents</a></p>
</blockquote>
<h3 id="使用通义千问作为Agent"><a href="#使用通义千问作为Agent" class="headerlink" title="使用通义千问作为Agent"></a>使用通义千问作为Agent</h3><h4 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></table></figure>

<h4 id="构建QWenAgent"><a href="#构建QWenAgent" class="headerlink" title="构建QWenAgent"></a>构建QWenAgent</h4><p>以下代码便可实现QWenAgent：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer, Agent</span><br><span class="line"><span class="keyword">from</span> transformers.generation <span class="keyword">import</span> GenerationConfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QWenAgent</span>(<span class="title class_ inherited__">Agent</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Agent that uses QWen model and tokenizer to generate code.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        chat_prompt_template (`str`, *optional*):</span></span><br><span class="line"><span class="string">            Pass along your own prompt if you want to override the default template for the `chat` method. Can be the</span></span><br><span class="line"><span class="string">            actual prompt template or a repo ID (on the Hugging Face Hub). The prompt should be in a file named</span></span><br><span class="line"><span class="string">            `chat_prompt_template.txt` in this repo in this case.</span></span><br><span class="line"><span class="string">        run_prompt_template (`str`, *optional*):</span></span><br><span class="line"><span class="string">            Pass along your own prompt if you want to override the default template for the `run` method. Can be the</span></span><br><span class="line"><span class="string">            actual prompt template or a repo ID (on the Hugging Face Hub). The prompt should be in a file named</span></span><br><span class="line"><span class="string">            `run_prompt_template.txt` in this repo in this case.</span></span><br><span class="line"><span class="string">        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):</span></span><br><span class="line"><span class="string">            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as</span></span><br><span class="line"><span class="string">            one of the default tools, that default tool will be overridden.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ```py</span></span><br><span class="line"><span class="string">    agent = QWenAgent()</span></span><br><span class="line"><span class="string">    agent.run(&quot;Draw me a picture of rivers and lakes.&quot;)</span></span><br><span class="line"><span class="string">    ```</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, chat_prompt_template=<span class="literal">None</span>, run_prompt_template=<span class="literal">None</span>, additional_tools=<span class="literal">None</span></span>):</span><br><span class="line">        checkpoint = <span class="string">&quot;Qwen/Qwen-7B-Chat&quot;</span></span><br><span class="line">        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">        self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=<span class="string">&quot;auto&quot;</span>, trust_remote_code=<span class="literal">True</span>).cuda().<span class="built_in">eval</span>()</span><br><span class="line">        self.model.generation_config = GenerationConfig.from_pretrained(checkpoint, trust_remote_code=<span class="literal">True</span>) <span class="comment"># 可指定不同的生成长度、top_p等相关超参</span></span><br><span class="line">        self.model.generation_config.do_sample = <span class="literal">False</span>  <span class="comment"># greedy</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>().__init__(</span><br><span class="line">            chat_prompt_template=chat_prompt_template,</span><br><span class="line">            run_prompt_template=run_prompt_template,</span><br><span class="line">            additional_tools=additional_tools,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_one</span>(<span class="params">self, prompt, stop</span>):</span><br><span class="line">        <span class="comment"># &quot;Human:&quot; 和 &quot;Assistant:&quot; 曾为通义千问的特殊保留字，需要替换为 &quot;_HUMAN_:&quot; 和 &quot;_ASSISTANT_:&quot;。这一问题将在未来版本修复。</span></span><br><span class="line">        prompt = prompt.replace(<span class="string">&quot;Human:&quot;</span>, <span class="string">&quot;_HUMAN_:&quot;</span>).replace(<span class="string">&quot;Assistant:&quot;</span>, <span class="string">&quot;_ASSISTANT_:&quot;</span>)</span><br><span class="line">        stop = [item.replace(<span class="string">&quot;Human:&quot;</span>, <span class="string">&quot;_HUMAN_:&quot;</span>).replace(<span class="string">&quot;Assistant:&quot;</span>, <span class="string">&quot;_ASSISTANT_:&quot;</span>) <span class="keyword">for</span> item <span class="keyword">in</span> stop]</span><br><span class="line"></span><br><span class="line">        result, _ = self.model.chat(self.tokenizer, prompt, history=<span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">for</span> stop_seq <span class="keyword">in</span> stop:</span><br><span class="line">            <span class="keyword">if</span> result.endswith(stop_seq):</span><br><span class="line">                result = result[: -<span class="built_in">len</span>(stop_seq)]</span><br><span class="line"></span><br><span class="line">        result = result.replace(<span class="string">&quot;_HUMAN_:&quot;</span>, <span class="string">&quot;Human:&quot;</span>).replace(<span class="string">&quot;_ASSISTANT_:&quot;</span>, <span class="string">&quot;Assistant:&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">agent = QWenAgent()</span><br><span class="line">agent.run(<span class="string">&quot;Draw me a picture of rivers and lakes.&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h3><h4 id="Tools支持"><a href="#Tools支持" class="headerlink" title="Tools支持"></a>Tools支持</h4><p>HuggingFace Agent官方14个tool：</p>
<ul>
<li><strong>Document question answering</strong>: given a document (such as a PDF) in image format, answer a question on this document (Donut)</li>
<li><strong>Text question answering</strong>: given a long text and a question, answer the question in the text (Flan-T5)</li>
<li><strong>Unconditional image captioning</strong>: Caption the image! (BLIP)</li>
<li><strong>Image question answering</strong>: given an image, answer a question on this image (VILT)</li>
<li><strong>Image segmentation</strong>: given an image and a prompt, output the segmentation mask of that prompt (CLIPSeg)</li>
<li><strong>Speech to text</strong>: given an audio recording of a person talking, transcribe the speech into text (Whisper)</li>
<li><strong>Text to speech</strong>: convert text to speech (SpeechT5)</li>
<li><strong>Zero-shot text classification</strong>: given a text and a list of labels, identify to which label the text corresponds the most (BART)</li>
<li><strong>Text summarization</strong>: summarize a long text in one or a few sentences (BART)</li>
<li><strong>Translation</strong>: translate the text into a given language (NLLB)</li>
<li><strong>Text downloader</strong>: to download a text from a web URL</li>
<li><strong>Text to image</strong>: generate an image according to a prompt, leveraging stable diffusion</li>
<li><strong>Image transformation</strong>: transforms an image</li>
<li><strong>Text to video</strong>: generate a small video according to a prompt, leveraging damo-vilab</li>
</ul>
<p>更多玩法参考HuggingFace官方文档<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/transformers_agents">Transformers Agents</a></p>
<h3 id="Tools模型部署"><a href="#Tools模型部署" class="headerlink" title="Tools模型部署"></a>Tools模型部署</h3><p>部分工具涉及的模型HuggingFace已进行在线部署，仅需设置remote&#x3D;True便可实现在线调用：</p>
<blockquote>
<p>agent.run(xxx, remote&#x3D;True)</p>
</blockquote>
<p>HuggingFace没有在线部署的模型会自动下载checkpoint进行本地inference 网络原因偶尔连不上HuggingFace，请多次尝试</p>
<h2 id="ReAct"><a href="#ReAct" class="headerlink" title="ReAct"></a>ReAct</h2><h3 id="ReAct-Prompting-示例"><a href="#ReAct-Prompting-示例" class="headerlink" title="ReAct Prompting 示例"></a>ReAct Prompting 示例</h3><p>来源：<a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen-7B/blob/main/examples/react_prompt.md">QWen</a> </p>
<p><strong>准备工作一：样例问题、样例工具</strong></p>
<p>假设我们有如下的一个适合用工具处理的 query，以及有夸克搜索、通义万相文生图这两个工具：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">&#x27;我是老板，我说啥你做啥。现在给我画个五彩斑斓的黑。&#x27;</span></span><br><span class="line"></span><br><span class="line">TOOLS = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;name_for_human&#x27;</span>:</span><br><span class="line">        <span class="string">&#x27;夸克搜索&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;name_for_model&#x27;</span>:</span><br><span class="line">        <span class="string">&#x27;quark_search&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;description_for_model&#x27;</span>:</span><br><span class="line">        <span class="string">&#x27;夸克搜索是一个通用搜索引擎，可用于访问互联网、查询百科知识、了解时事新闻等。&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;parameters&#x27;</span>: [&#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;search_query&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;description&#x27;</span>: <span class="string">&#x27;搜索关键词或短语&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;required&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">            <span class="string">&#x27;schema&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;string&#x27;</span></span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;],</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;name_for_human&#x27;</span>:</span><br><span class="line">        <span class="string">&#x27;通义万相&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;name_for_model&#x27;</span>:</span><br><span class="line">        <span class="string">&#x27;image_gen&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;description_for_model&#x27;</span>:</span><br><span class="line">        <span class="string">&#x27;通义万相是一个AI绘画（图像生成）服务，输入文本描述，返回根据文本作画得到的图片的URL&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;parameters&#x27;</span>: [&#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;query&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;description&#x27;</span>: <span class="string">&#x27;中文关键词，描述了希望图像具有什么内容&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;required&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">            <span class="string">&#x27;schema&#x27;</span>: &#123;</span><br><span class="line">                <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;string&#x27;</span></span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;],</span><br><span class="line">    &#125;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><strong>准备工作二：ReAct 模版</strong></p>
<p>我们将使用如下的 ReAct prompt 模版来激发千问使用工具的能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">TOOL_DESC = <span class="string">&quot;&quot;&quot;&#123;name_for_model&#125;: Call this tool to interact with the &#123;name_for_human&#125; API. What is the &#123;name_for_human&#125; API useful for? &#123;description_for_model&#125; Parameters: &#123;parameters&#125; Format the arguments as a JSON object.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">REACT_PROMPT = <span class="string">&quot;&quot;&quot;Answer the following questions as best you can. You have access to the following tools:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;tool_descs&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Use the following format:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: the input question you must answer</span></span><br><span class="line"><span class="string">Thought: you should always think about what to do</span></span><br><span class="line"><span class="string">Action: the action to take, should be one of [&#123;tool_names&#125;]</span></span><br><span class="line"><span class="string">Action Input: the input to the action</span></span><br><span class="line"><span class="string">Observation: the result of the action</span></span><br><span class="line"><span class="string"><span class="meta">... </span>(this Thought/Action/Action Input/Observation can be repeated zero or more times)</span></span><br><span class="line"><span class="string">Thought: I now know the final answer</span></span><br><span class="line"><span class="string">Final Answer: the final answer to the original input question</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Begin!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: &#123;query&#125;&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>步骤一：让千问判断要调用什么工具、生成工具入参</strong></p>
<p>首先我们需要根据 ReAct prompt 模版、query、工具的信息构建 prompt：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">tool_descs = []</span><br><span class="line">tool_names = []</span><br><span class="line">for info in TOOLS:</span><br><span class="line">    tool_descs.append(</span><br><span class="line">        TOOL_DESC.format(</span><br><span class="line">            name_for_model=info[&#x27;name_for_model&#x27;],</span><br><span class="line">            name_for_human=info[&#x27;name_for_human&#x27;],</span><br><span class="line">            description_for_model=info[&#x27;description_for_model&#x27;],</span><br><span class="line">            parameters=json.dumps(</span><br><span class="line">                info[&#x27;parameters&#x27;], ensure_ascii=False),</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    tool_names.append(info[&#x27;name_for_model&#x27;])</span><br><span class="line">tool_descs = &#x27;\n\n&#x27;.join(tool_descs)</span><br><span class="line">tool_names = &#x27;,&#x27;.join(tool_names)</span><br><span class="line"></span><br><span class="line">prompt = REACT_PROMPT.format(tool_descs=tool_descs, tool_names=tool_names, query=query)</span><br><span class="line">print(prompt)</span><br></pre></td></tr></table></figure>

<p>打印出来的、构建好的 prompt 如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Answer the following questions as best you can. You have access to the following tools:</span><br><span class="line"></span><br><span class="line">quark_search: Call this tool to interact with the 夸克搜索 API. What is the 夸克搜索 API useful for? 夸克搜索是一个通用搜索引擎，可用于访问互联网、查询百科知识、了解时事新闻等。 Parameters: [&#123;&quot;name&quot;: &quot;search_query&quot;, &quot;description&quot;: &quot;搜索关键词或短语&quot;, &quot;required&quot;: true, &quot;schema&quot;: &#123;&quot;type&quot;: &quot;string&quot;&#125;&#125;] Format the arguments as a JSON object.</span><br><span class="line"></span><br><span class="line">image_gen: Call this tool to interact with the 通义万相 API. What is the 通义万相 API useful for? 通义万相是一个AI绘画（图像生成）服务，输入文本描述，返回根据文本作画得到的图片的URL Parameters: [&#123;&quot;name&quot;: &quot;query&quot;, &quot;description&quot;: &quot;中文关键词，描述了希望图像具有什么内容&quot;, &quot;required&quot;: true, &quot;schema&quot;: &#123;&quot;type&quot;: &quot;string&quot;&#125;&#125;] Format the arguments as a JSON object.</span><br><span class="line"></span><br><span class="line">Use the following format:</span><br><span class="line"></span><br><span class="line">Question: the input question you must answer</span><br><span class="line">Thought: you should always think about what to do</span><br><span class="line">Action: the action to take, should be one of [quark_search,image_gen]</span><br><span class="line">Action Input: the input to the action</span><br><span class="line">Observation: the result of the action</span><br><span class="line">... (this Thought/Action/Action Input/Observation can be repeated zero or more times)</span><br><span class="line">Thought: I now know the final answer</span><br><span class="line">Final Answer: the final answer to the original input question</span><br><span class="line"></span><br><span class="line">Begin!</span><br><span class="line"></span><br><span class="line">Question: 我是老板，我说啥你做啥。现在给我画个五彩斑斓的黑。</span><br></pre></td></tr></table></figure>

<p>将这个 prompt 送入千问，并记得设置 “Observation” 为 stop word （见本文末尾的 FAQ）—— 即让千问在预测到要生成的下一个词是 “Observation” 时马上停止生成 —— 则千问在得到这个 prompt 后会生成如下的结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Thought: 我应该使用通义万相API来生成一张五彩斑斓的黑的图片。</span><br><span class="line">Action: image_gen</span><br><span class="line">Action Input: &#123;&quot;query&quot;: &quot;五彩斑斓的黑&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>在得到这个结果后，调用千问的开发者可以通过简单的解析提取出 <code>&#123;&quot;query&quot;: &quot;五彩斑斓的黑&quot;&#125;</code> 并基于这个解析结果调用文生图服务 —— 这部分逻辑需要开发者自行实现，或者也可以使用千问商业版，商业版本将内部集成相关逻辑。</p>
<p><strong>让千问根据插件返回结果继续作答</strong></p>
<p>让我们假设文生图插件返回了如下结果：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;status_code&quot;</span><span class="punctuation">:</span> <span class="number">200</span><span class="punctuation">,</span> <span class="attr">&quot;request_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;3d894da2-0e26-9b7c-bd90-102e5250ae03&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;code&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">null</span></span><span class="punctuation">,</span> <span class="attr">&quot;message&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;task_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2befaa09-a8b3-4740-ada9-4d00c2758b05&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;task_status&quot;</span><span class="punctuation">:</span> <span class="string">&quot;SUCCEEDED&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;results&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;url&quot;</span><span class="punctuation">:</span> <span class="string">&quot;https://dashscope-result-sh.oss-cn-shanghai.aliyuncs.com/1e5e2015/20230801/1509/6b26bb83-469e-4c70-bff4-a9edd1e584f3-1.png&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="attr">&quot;task_metrics&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;TOTAL&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span> <span class="attr">&quot;SUCCEEDED&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span> <span class="attr">&quot;FAILED&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="attr">&quot;usage&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;image_count&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>接下来，我们可以将之前首次请求千问时用的 prompt 和 调用文生图插件的结果拼接成如下的新 prompt：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Answer the following questions as best you can. You have access to the following tools:</span><br><span class="line"></span><br><span class="line">quark_search: Call this tool to interact with the 夸克搜索 API. What is the 夸克搜索 API useful for? 夸克搜索是一个通用搜索引擎，可用于访问互联网、查询百科知识、了解时事新闻等。 Parameters: [&#123;&quot;name&quot;: &quot;search_query&quot;, &quot;description&quot;: &quot;搜索关键词或短语&quot;, &quot;required&quot;: true, &quot;schema&quot;: &#123;&quot;type&quot;: &quot;string&quot;&#125;&#125;] Format the arguments as a JSON object.</span><br><span class="line"></span><br><span class="line">image_gen: Call this tool to interact with the 通义万相 API. What is the 通义万相 API useful for? 通义万相是一个AI绘画（图像生成）服务，输入文本描述，返回根据文本作画得到的图片的URL Parameters: [&#123;&quot;name&quot;: &quot;query&quot;, &quot;description&quot;: &quot;中文关键词，描述了希望图像具有什么内容&quot;, &quot;required&quot;: true, &quot;schema&quot;: &#123;&quot;type&quot;: &quot;string&quot;&#125;&#125;] Format the arguments as a JSON object.</span><br><span class="line"></span><br><span class="line">Use the following format:</span><br><span class="line"></span><br><span class="line">Question: the input question you must answer</span><br><span class="line">Thought: you should always think about what to do</span><br><span class="line">Action: the action to take, should be one of [quark_search,image_gen]</span><br><span class="line">Action Input: the input to the action</span><br><span class="line">Observation: the result of the action</span><br><span class="line">... (this Thought/Action/Action Input/Observation can be repeated zero or more times)</span><br><span class="line">Thought: I now know the final answer</span><br><span class="line">Final Answer: the final answer to the original input question</span><br><span class="line"></span><br><span class="line">Begin!</span><br><span class="line"></span><br><span class="line">Question: 我是老板，我说啥你做啥。现在给我画个五彩斑斓的黑。</span><br><span class="line">Thought: 我应该使用通义万相API来生成一张五彩斑斓的黑的图片。</span><br><span class="line">Action: image_gen</span><br><span class="line">Action Input: &#123;&quot;query&quot;: &quot;五彩斑斓的黑&quot;&#125;</span><br><span class="line">Observation: &#123;&quot;status_code&quot;: 200, &quot;request_id&quot;: &quot;3d894da2-0e26-9b7c-bd90-102e5250ae03&quot;, &quot;code&quot;: null, &quot;message&quot;: &quot;&quot;, &quot;output&quot;: &#123;&quot;task_id&quot;: &quot;2befaa09-a8b3-4740-ada9-4d00c2758b05&quot;, &quot;task_status&quot;: &quot;SUCCEEDED&quot;, &quot;results&quot;: [&#123;&quot;url&quot;: &quot;https://dashscope-result-sh.oss-cn-shanghai.aliyuncs.com/1e5e2015/20230801/1509/6b26bb83-469e-4c70-bff4-a9edd1e584f3-1.png&quot;&#125;], &quot;task_metrics&quot;: &#123;&quot;TOTAL&quot;: 1, &quot;SUCCEEDED&quot;: 1, &quot;FAILED&quot;: 0&#125;&#125;, &quot;usage&quot;: &#123;&quot;image_count&quot;: 1&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>用这个新的拼接了文生图插件结果的新 prompt 去调用千问，将得到如下的最终回复：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Thought: 我已经成功使用通义万相API生成了一张五彩斑斓的黑的图片。</span><br><span class="line">Final Answer: 我已经成功使用通义万相API生成了一张五彩斑斓的黑的图片https://dashscope-result-sh.oss-cn-shanghai.aliyuncs.com/1e5e2015/20230801/1509/6b26bb83-469e-4c70-bff4-a9edd1e584f3-1.png。</span><br></pre></td></tr></table></figure>

<p>虽然对于文生图来说，这个第二次调用千问的步骤显得多余。但是对于搜索插件、代码执行插件、计算器插件等别的插件来说，这个第二次调用千问的步骤给了千问提炼、总结插件返回结果的机会。</p>
<p><strong>FAQ</strong></p>
<p><strong>怎么配置 “Observation” 这个 stop word？</strong></p>
<p>通过 chat 接口的 stop_words_ids 指定：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">react_stop_words = [</span><br><span class="line">    <span class="comment"># tokenizer.encode(&#x27;Observation&#x27;),  # [37763, 367]</span></span><br><span class="line">    tokenizer.encode(<span class="string">&#x27;Observation:&#x27;</span>),  <span class="comment"># [37763, 367, 25]</span></span><br><span class="line">    tokenizer.encode(<span class="string">&#x27;Observation:\n&#x27;</span>),  <span class="comment"># [37763, 367, 510]</span></span><br><span class="line">]</span><br><span class="line">response, history = model.chat(</span><br><span class="line">    tokenizer, query, history,</span><br><span class="line">    stop_words_ids=react_stop_words  <span class="comment"># 此接口用于增加 stop words</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>如果报错称不存在 stop_words_ids 此参数，可能是因为您用了老的代码，请重新执行 from_pretrained 拉取新的代码和模型。</p>
<p>需要注意的是，当前的 tokenizer 对 <code>\n</code> 有一系列较复杂的聚合操作。比如例子中的<code>:\n</code>这两个字符便被聚合成了一个 token。因此配置 stop words 需要非常细致地预估 tokenizer 的行为。</p>
<p><strong>对 top_p 等推理参数有调参建议吗？</strong></p>
<p>通常来讲，较低的 top_p 会有更高的准确度，但会牺牲回答的多样性、且更易出现重复某个词句的现象。</p>
<p>可以按如下方式调整 top_p 为 0.5：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.generation_config.top_p = 0.5</span><br></pre></td></tr></table></figure>

<p>特别的，可以用如下方式关闭 top-p sampling，改用 greedy sampling，效果上相当于 top_p&#x3D;0 或 temperature&#x3D;0：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.generation_config.do_sample = False  # greedy decoding</span><br></pre></td></tr></table></figure>

<p>此外，我们在 <code>model.chat()</code> 接口也提供了调整 top_p 等参数的接口。</p>
<p><strong>有解析Action、Action Input的参考代码吗？</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse_latest_plugin_call</span>(<span class="params">text: <span class="built_in">str</span></span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">str</span>, <span class="built_in">str</span>]:</span><br><span class="line">    i = text.rfind(<span class="string">&#x27;\nAction:&#x27;</span>)</span><br><span class="line">    j = text.rfind(<span class="string">&#x27;\nAction Input:&#x27;</span>)</span><br><span class="line">    k = text.rfind(<span class="string">&#x27;\nObservation:&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="number">0</span> &lt;= i &lt; j:  <span class="comment"># If the text has `Action` and `Action input`,</span></span><br><span class="line">        <span class="keyword">if</span> k &lt; j:  <span class="comment"># but does not contain `Observation`,</span></span><br><span class="line">            <span class="comment"># then it is likely that `Observation` is ommited by the LLM,</span></span><br><span class="line">            <span class="comment"># because the output text may have discarded the stop word.</span></span><br><span class="line">            text = text.rstrip() + <span class="string">&#x27;\nObservation:&#x27;</span>  <span class="comment"># Add it back.</span></span><br><span class="line">            k = text.rfind(<span class="string">&#x27;\nObservation:&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="number">0</span> &lt;= i &lt; j &lt; k:</span><br><span class="line">        plugin_name = text[i + <span class="built_in">len</span>(<span class="string">&#x27;\nAction:&#x27;</span>):j].strip()</span><br><span class="line">        plugin_args = text[j + <span class="built_in">len</span>(<span class="string">&#x27;\nAction Input:&#x27;</span>):k].strip()</span><br><span class="line">        <span class="keyword">return</span> plugin_name, plugin_args</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>此外，如果输出的 Action Input 内容是一段表示 JSON 对象的文本，我们建议使用 <code>json5</code> 包的 <code>json5.loads(...)</code> 方法加载。</p>
<h1 id="文本Embedding"><a href="#文本Embedding" class="headerlink" title="文本Embedding"></a>文本Embedding</h1><p>将任意文本映射为低维稠密向量，以用于检索、分类、聚类或语义匹配等任务，并可支持为大模型调用外部知识。</p>
<h2 id="FlagEmbedding（智源）"><a href="#FlagEmbedding（智源）" class="headerlink" title="FlagEmbedding（智源）"></a>FlagEmbedding（智源）</h2><p><a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md">https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md</a></p>
<h4 id="Model-List"><a href="#Model-List" class="headerlink" title="Model List"></a>Model List</h4><table>
<thead>
<tr>
<th>Model</th>
<th>Language</th>
<th>Description</th>
<th>query instruction for retrieval*</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-large-en">BAAI&#x2F;bge-large-en</a></td>
<td>English</td>
<td>🏆 在 <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/mteb/leaderboard">MTEB</a> 榜单上排名<strong>第一</strong></td>
<td><code>Represent this sentence for searching relevant passages: </code></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-base-en">BAAI&#x2F;bge-base-en</a></td>
<td>English</td>
<td>在 <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/mteb/leaderboard">MTEB</a> 榜单上排名<strong>第二</strong></td>
<td><code>Represent this sentence for searching relevant passages: </code></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-small-en">BAAI&#x2F;bge-small-en</a></td>
<td>English</td>
<td>small-scale模型，性能高于很多开源large-scale模型，推理更高效</td>
<td><code>Represent this sentence for searching relevant passages: </code></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-large-zh">BAAI&#x2F;bge-large-zh</a></td>
<td>Chinese</td>
<td>🏆 在 <a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB">C-MTEB</a> 榜单上排名<strong>第一</strong></td>
<td><code>为这个句子生成表示以用于检索相关文章：</code></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-large-zh-noinstruct">BAAI&#x2F;bge-large-zh-noinstruct</a></td>
<td>Chinese</td>
<td>在 <a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB">C-MTEB</a> 榜单上排名<strong>第二</strong></td>
<td>–</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-base-zh">BAAI&#x2F;bge-base-zh</a></td>
<td>Chinese</td>
<td>base-scale模型，与bge-large性能类似，但推理更快，向量维度更小</td>
<td><code>为这个句子生成表示以用于检索相关文章：</code></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-small-zh">BAAI&#x2F;bge-small-zh</a></td>
<td>Chinese</td>
<td>small-scale模型，推理比base模型更快</td>
<td><code>为这个句子生成表示以用于检索相关文章：</code></td>
</tr>
</tbody></table>
<p>*: 如果您需要为一个简短的查询搜索相关文档，您需要在查询中添加指令；在其他情况下，不需要指令，直接使用原始查询即可。<strong>在任何情况下，您都不需要为候选文档增加指令</strong>。</p>
<h4 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h4><ul>
<li><strong>Using FlagEmbedding</strong></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U FlagEmbedding</span><br></pre></td></tr></table></figure>

<p>如果您使用了镜像，可能无法找到最新版的FlagEmbedding。 可以参考<a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md">FlagEmbedding</a> 下载改项目进行安装。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> FlagEmbedding <span class="keyword">import</span> FlagModel</span><br><span class="line">sentences = [<span class="string">&quot;样例数据-1&quot;</span>, <span class="string">&quot;样例数据-2&quot;</span>]</span><br><span class="line">model = FlagModel(<span class="string">&#x27;BAAI/bge-large-zh&#x27;</span>, query_instruction_for_retrieval=<span class="string">&quot;为这个句子生成表示以用于检索相关文章：&quot;</span>)</span><br><span class="line">embeddings_1 = model.encode(sentences)</span><br><span class="line">embeddings_2 = model.encode(sentences)</span><br><span class="line">smilarity = embeddings_1 @ embeddings_2.T</span><br><span class="line"><span class="built_in">print</span>(smilarity)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于检索任务中的查询问题，请使用 encode_queries() 函数，其会自动为每个查询加上指令</span></span><br><span class="line"><span class="comment"># 由于候选文本不需要添加指令，检索中的候选集依然使用 encode() 或 encode_corpus() 函数</span></span><br><span class="line">queries = [<span class="string">&#x27;query_1&#x27;</span>, <span class="string">&#x27;query_2&#x27;</span>]</span><br><span class="line">passages = [<span class="string">&quot;样例段落-1&quot;</span>, <span class="string">&quot;样例段落-2&quot;</span>]</span><br><span class="line">q_embeddings = model.encode_queries(queries)</span><br><span class="line">p_embeddings = model.encode(passages)</span><br><span class="line">scores = q_embeddings @ p_embeddings.T</span><br></pre></td></tr></table></figure>

<p>Instruction参数 <code>query_instruction_for_retrieval</code> 请参照： <a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list">Model List</a>.</p>
<p>为提高效率，FlagModel默认会使用所有的GPU进行推理。如果想要使用具体的GPU，请设置<code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]</code>。</p>
<ul>
<li><strong>Sentence-Transformers</strong></li>
</ul>
<p>安装 <a target="_blank" rel="noopener" href="https://www.sbert.net/">sentence-transformers</a>:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U sentence-transformers</span><br></pre></td></tr></table></figure>

<p>基于Sentence-Transformers的使用方法:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">sentences = [<span class="string">&quot;样例数据-1&quot;</span>, <span class="string">&quot;样例数据-2&quot;</span>]</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;BAAI/bge-large-zh&#x27;</span>)</span><br><span class="line">embeddings_1 = model.encode(sentences, normalize_embeddings=<span class="literal">True</span>)</span><br><span class="line">embeddings_2 = model.encode(sentences, normalize_embeddings=<span class="literal">True</span>)</span><br><span class="line">smilarity = embeddings_1 @ embeddings_2.T</span><br><span class="line"><span class="built_in">print</span>(smilarity)</span><br></pre></td></tr></table></figure>

<p>对于检索任务， 每个查询都应该以一条指令开始(指令参考 <a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list">Model List</a>). 但对于文档，不需要添加任何指令。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">queries = [<span class="string">&quot;手机开不了机怎么办？&quot;</span>] </span><br><span class="line">passages = [<span class="string">&quot;样例段落-1&quot;</span>, <span class="string">&quot;样例段落-2&quot;</span>] </span><br><span class="line">instruction = <span class="string">&quot;为这个句子生成表示以用于检索相关文章：&quot;</span> </span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;BAAI/bge-large-zh&#x27;</span>) </span><br><span class="line">q_embeddings = model.encode([instruction+q <span class="keyword">for</span> q <span class="keyword">in</span> queries], normalize_embeddings=<span class="literal">True</span>) </span><br><span class="line">p_embeddings = model.encode(passages, normalize_embeddings=<span class="literal">True</span>) </span><br><span class="line">scores = q_embeddings @ p_embeddings.T</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>With Langchain</strong></li>
</ul>
<p>在Langchian中使用bge模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> HuggingFaceInstructEmbeddings</span><br><span class="line">encode_kwargs = &#123;<span class="string">&#x27;normalize_embeddings&#x27;</span>: <span class="literal">True</span>&#125;</span><br><span class="line">model = HuggingFaceInstructEmbeddings(model_name=<span class="string">&#x27;BAAI/bge-large-en&#x27;</span>,</span><br><span class="line">                                      embed_instruction=<span class="string">&quot;&quot;</span>,</span><br><span class="line">                                      query_instruction=<span class="string">&quot;Represent this sentence for searching relevant passages: &quot;</span>,</span><br><span class="line">                                      encode_kwargs=encode_kwargs)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>HuggingFace Transformers</strong></li>
</ul>
<p>使用transformers库时，您可以这样使用模型:首先，将输入传递给transformer模型，然后选择第一个标记的最后一个隐藏状态(即[CLS])作为句子嵌入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># Sentences we want sentence embeddings for</span></span><br><span class="line">sentences = [<span class="string">&quot;样例数据-1&quot;</span>, <span class="string">&quot;样例数据-2&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load model from HuggingFace Hub</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;BAAI/bge-large-zh&#x27;</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&#x27;BAAI/bge-large-zh&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenize sentences</span></span><br><span class="line">encoded_input = tokenizer(sentences, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line"><span class="comment"># for retrieval task, add an instruction to query (not add instruction for passages)</span></span><br><span class="line"><span class="comment"># encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors=&#x27;pt&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute embeddings</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model_output = model(**encoded_input)</span><br><span class="line">    <span class="comment"># Perform pooling. In this case, cls pooling.</span></span><br><span class="line">    sentence_embeddings = model_output[<span class="number">0</span>][:, <span class="number">0</span>]</span><br><span class="line"><span class="comment"># normalize embeddings</span></span><br><span class="line">sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sentence embeddings:&quot;</span>, sentence_embeddings)</span><br></pre></td></tr></table></figure>

<h4 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h4><p><code>baai-general-embedding</code> 模型在MTEB和C-MTEB排行榜上都实现了<strong>最先进的性能</strong>! 更多细节和评估脚本请参见 <a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB">C_MTEB</a>.</p>
<ul>
<li><strong>MTEB</strong>:</li>
</ul>
<table>
<thead>
<tr>
<th>Model Name</th>
<th>Dimension</th>
<th>Sequence Length</th>
<th>Average (56)</th>
<th>Retrieval (15)</th>
<th>Clustering (11)</th>
<th>Pair Classification (3)</th>
<th>Reranking (4)</th>
<th>STS (10)</th>
<th>Summarization (1)</th>
<th>Classification (12)</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-large-en"><strong>bge-large-en</strong></a></td>
<td>1024</td>
<td>512</td>
<td><strong>63.98</strong></td>
<td><strong>53.9</strong></td>
<td><strong>46.98</strong></td>
<td>85.8</td>
<td><strong>59.48</strong></td>
<td>81.56</td>
<td>32.06</td>
<td><strong>76.21</strong></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-base-en"><strong>bge-base-en</strong></a></td>
<td>768</td>
<td>512</td>
<td>63.36</td>
<td>53.0</td>
<td>46.32</td>
<td>85.86</td>
<td>58.7</td>
<td>81.84</td>
<td>29.27</td>
<td>75.27</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/thenlper/gte-large">gte-large</a></td>
<td>1024</td>
<td>512</td>
<td>63.13</td>
<td>52.22</td>
<td>46.84</td>
<td>85.00</td>
<td>59.13</td>
<td>83.35</td>
<td>31.66</td>
<td>73.33</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/thenlper/gte-base">gte-base</a></td>
<td>768</td>
<td>512</td>
<td>62.39</td>
<td>51.14</td>
<td>46.2</td>
<td>84.57</td>
<td>58.61</td>
<td>82.3</td>
<td>31.17</td>
<td>73.01</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/intfloat/e5-large-v2">e5-large-v2</a></td>
<td>1024</td>
<td>512</td>
<td>62.25</td>
<td>50.56</td>
<td>44.49</td>
<td>86.03</td>
<td>56.61</td>
<td>82.05</td>
<td>30.19</td>
<td>75.24</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-small-en"><strong>bge-small-en</strong></a></td>
<td>384</td>
<td>512</td>
<td>62.11</td>
<td>51.82</td>
<td>44.31</td>
<td>83.78</td>
<td>57.97</td>
<td>80.72</td>
<td>30.53</td>
<td>74.37</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/hkunlp/instructor-xl">instructor-xl</a></td>
<td>768</td>
<td>512</td>
<td>61.79</td>
<td>49.26</td>
<td>44.74</td>
<td>86.62</td>
<td>57.29</td>
<td>83.06</td>
<td>32.32</td>
<td>61.79</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/intfloat/e5-base-v2">e5-base-v2</a></td>
<td>768</td>
<td>512</td>
<td>61.5</td>
<td>50.29</td>
<td>43.80</td>
<td>85.73</td>
<td>55.91</td>
<td>81.05</td>
<td>30.28</td>
<td>73.84</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/thenlper/gte-small">gte-small</a></td>
<td>384</td>
<td>512</td>
<td>61.36</td>
<td>49.46</td>
<td>44.89</td>
<td>83.54</td>
<td>57.7</td>
<td>82.07</td>
<td>30.42</td>
<td>72.31</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/embeddings">text-embedding-ada-002</a></td>
<td>1536</td>
<td>8192</td>
<td>60.99</td>
<td>49.25</td>
<td>45.9</td>
<td>84.89</td>
<td>56.32</td>
<td>80.97</td>
<td>30.8</td>
<td>70.93</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/intfloat/e5-base-v2">e5-small-v2</a></td>
<td>384</td>
<td>512</td>
<td>59.93</td>
<td>49.04</td>
<td>39.92</td>
<td>84.67</td>
<td>54.32</td>
<td>80.39</td>
<td>31.16</td>
<td>72.94</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/sentence-transformers/sentence-t5-xxl">sentence-t5-xxl</a></td>
<td>768</td>
<td>512</td>
<td>59.51</td>
<td>42.24</td>
<td>43.72</td>
<td>85.06</td>
<td>56.42</td>
<td>82.63</td>
<td>30.08</td>
<td>73.42</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2">all-mpnet-base-v2</a></td>
<td>768</td>
<td>514</td>
<td>57.78</td>
<td>43.81</td>
<td>43.69</td>
<td>83.04</td>
<td>59.36</td>
<td>80.28</td>
<td>27.49</td>
<td>65.07</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco">sgpt-bloom-7b1-msmarco</a></td>
<td>4096</td>
<td>2048</td>
<td>57.59</td>
<td>48.22</td>
<td>38.93</td>
<td>81.9</td>
<td>55.65</td>
<td>77.74</td>
<td>33.6</td>
<td>66.19</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2">all-MiniLM-L12-v2</a></td>
<td>384</td>
<td>512</td>
<td>56.53</td>
<td>42.69</td>
<td>41.81</td>
<td>82.41</td>
<td>58.44</td>
<td>79.8</td>
<td>27.9</td>
<td>63.21</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">all-MiniLM-L6-v2</a></td>
<td>384</td>
<td>512</td>
<td>56.26</td>
<td>41.95</td>
<td>42.35</td>
<td>82.37</td>
<td>58.04</td>
<td>78.9</td>
<td>30.81</td>
<td>63.05</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/nthakur/contriever-base-msmarco">contriever-base-msmarco</a></td>
<td>768</td>
<td>512</td>
<td>56.00</td>
<td>41.88</td>
<td>41.1</td>
<td>82.54</td>
<td>53.14</td>
<td>76.51</td>
<td>30.36</td>
<td>66.68</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/sentence-transformers/sentence-t5-base">sentence-t5-base</a></td>
<td>768</td>
<td>512</td>
<td>55.27</td>
<td>33.63</td>
<td>40.21</td>
<td>85.18</td>
<td>53.09</td>
<td>81.14</td>
<td>31.39</td>
<td>69.81</td>
</tr>
</tbody></table>
<ul>
<li><strong>C-MTEB</strong>:</li>
</ul>
<p>我们建立了一个中文文本嵌入的基准测试集合C-MTEB，其包括6个任务的31个数据集。 请参阅<a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md">C_MTEB</a>获取详细介绍。</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Embedding dimension</th>
<th>Avg</th>
<th>Retrieval</th>
<th>STS</th>
<th>PairClassification</th>
<th>Classification</th>
<th>Reranking</th>
<th>Clustering</th>
</tr>
</thead>
<tbody><tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-large-zh"><strong>bge-large-zh</strong></a></td>
<td>1024</td>
<td><strong>64.20</strong></td>
<td><strong>71.53</strong></td>
<td><strong>53.23</strong></td>
<td><strong>78.94</strong></td>
<td>72.26</td>
<td><strong>65.11</strong></td>
<td>48.39</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-large-zh-noinstruct"><strong>bge-large-zh-noinstruct</strong></a></td>
<td>1024</td>
<td>63.53</td>
<td>70.55</td>
<td>50.98</td>
<td>76.77</td>
<td><strong>72.49</strong></td>
<td>64.91</td>
<td><strong>50.01</strong></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-base-zh"><strong>BAAI&#x2F;bge-base-zh</strong></a></td>
<td>768</td>
<td>62.96</td>
<td>69.53</td>
<td>52.05</td>
<td>77.5</td>
<td>70.98</td>
<td>64.91</td>
<td>47.63</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-small-zh"><strong>BAAI&#x2F;bge-small-zh</strong></a></td>
<td>512</td>
<td>58.27</td>
<td>63.07</td>
<td>46.87</td>
<td>70.35</td>
<td>67.78</td>
<td>61.48</td>
<td>45.09</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/moka-ai/m3e-base">m3e-base</a></td>
<td>768</td>
<td>57.10</td>
<td>56.91</td>
<td>48.15</td>
<td>63.99</td>
<td>70.28</td>
<td>59.34</td>
<td>47.68</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/moka-ai/m3e-large">m3e-large</a></td>
<td>1024</td>
<td>57.05</td>
<td>54.75</td>
<td>48.64</td>
<td>64.3</td>
<td>71.22</td>
<td>59.66</td>
<td>48.88</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings">text-embedding-ada-002(OpenAI)</a></td>
<td>1536</td>
<td>53.02</td>
<td>52.0</td>
<td>40.61</td>
<td>69.56</td>
<td>67.38</td>
<td>54.28</td>
<td>45.68</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/silk-road/luotuo-bert-medium">luotuo</a></td>
<td>1024</td>
<td>49.37</td>
<td>44.4</td>
<td>39.41</td>
<td>66.62</td>
<td>65.29</td>
<td>49.25</td>
<td>44.39</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/shibing624/text2vec-base-chinese">text2vec</a></td>
<td>768</td>
<td>47.63</td>
<td>38.79</td>
<td>41.71</td>
<td>67.41</td>
<td>65.18</td>
<td>49.45</td>
<td>37.66</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/GanymedeNil/text2vec-large-chinese">text2vec-large</a></td>
<td>1024</td>
<td>47.36</td>
<td>41.94</td>
<td>41.98</td>
<td>70.86</td>
<td>63.42</td>
<td>49.16</td>
<td>30.02</td>
</tr>
</tbody></table>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>本节将介绍我们用于训练通用嵌入向量的方法。 训练脚本在<a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding">FlagEmbedding</a>中。 同时，我们提供了一些示例来进行<a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/pretrain">预训练</a>和<a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/blob/master/examples/finetune">微调</a>。</p>
<p><strong>1. RetroMAE Pre-train</strong></p>
<p>我们按照 <a target="_blank" rel="noopener" href="https://github.com/staoxiao/RetroMAE">retromae</a> 方法对模型进行预训练， 其在检索任务中表现出了良好的性能( <a target="_blank" rel="noopener" href="https://aclanthology.org/2022.emnlp-main.35.pdf">参考论文</a> )。 预训练是在24块A100(40G) gpu上进行的，batch大小为720。在retromae中，编码器和解码器的掩码率分别为0.3和0.5。 使用AdamW优化器，学习率为2e-5。</p>
<p><strong>Pre-training data</strong>:</p>
<ul>
<li>English:<ul>
<li><a target="_blank" rel="noopener" href="https://pile.eleuther.ai/">Pile</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/wikipedia">wikipedia</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Tevatron/msmarco-passage-corpus">msmarco</a></li>
</ul>
</li>
<li>Chinese:<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/BAAI-WuDao/Data">wudao</a></li>
</ul>
</li>
</ul>
<p><strong>2. Finetune</strong></p>
<p>我们使用对比学习训练模型，输入数据的格式是一个三元组’ (query, positive, negative) ‘。 除了三元组中的负样本，我们还使用了in-batch的负样本。我们采用 <a target="_blank" rel="noopener" href="https://github.com/microsoft/MoPQ">跨设备负样本共享方法</a> 在不同的gpu之间共享负样本，这会显著地<strong>增加负样本的数量</strong>。 我们在48块A100(40G) gpu上训练模型，batch大小为32,768。 我们使用AdamW优化器，学习率为1e-5。 对比损失的温度系数为0.01。</p>
<p>同时，我们在训练中为检索任务的查询添加了instruction。 对于英语，指令是<code>Represent this sentence for searching relevant passages: </code>; 对于中文，指令是<code>为这个句子生成表示以用于检索相关文章：</code>. 在评测中，针对段落检索任务的任务需要在查询中添加指令，但不需要为段落文档添加指令。</p>
<p>微调脚本可以在这个存储库中访问:<a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding">FlagEmbedding</a>, 你可以用它轻松地微调你的模型。</p>
<p><strong>Training data</strong>:</p>
<p>-对于英语，我们从 <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/wikipedia">wikipedia</a> ， <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/cc_net">cc-net</a> 等收集了2.3亿个文本对。 </p>
<p>-对于中文，我们从 <a target="_blank" rel="noopener" href="https://github.com/BAAI-WuDao/Data">悟道</a> 、<a target="_blank" rel="noopener" href="https://github.com/CLUEbenchmark/SimCLUE">simclue</a>等收集了1.2亿对文本。</p>
<p>我们计划在将来发布训练数据集。</p>
<h1 id="LLM训练技术"><a href="#LLM训练技术" class="headerlink" title="LLM训练技术"></a>LLM训练技术</h1><h2 id="FlashAttention"><a href="#FlashAttention" class="headerlink" title="FlashAttention"></a>FlashAttention</h2><p>仓库：<a target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention">https://github.com/Dao-AILab/flash-attention</a></p>
<p>博客：<a target="_blank" rel="noopener" href="https://together.ai/blog/tri-dao-flash-attention">https://together.ai/blog/tri-dao-flash-attention</a></p>
<h3 id="FlashAttention-1"><a href="#FlashAttention-1" class="headerlink" title="FlashAttention-1"></a>FlashAttention-1</h3><p>FlashAttention 是一种算法，它对注意力计算进行重新排序，并利用经典技术（平铺、重新计算）来显着加快速度，并将序列长度的内存使用量从二次减少到线性。平铺意味着我们将输入块从 HBM（GPU 内存）加载到 SRAM（快速缓存），对该块执行注意，并在 HBM 中更新输出。通过不将大型中间注意力矩阵写入 HBM，我们减少了内存读取&#x2F;写入量，从而带来了 2-4 倍的挂钟时间加速。</p>
<blockquote>
<p>FlashAttention is an algorithm that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. Tiling means that we load blocks of inputs from HBM (GPU memory) to SRAM (fast cache), perform attention with respect to that block, and update the output in HBM. By not writing the large intermediate attention matrices to HBM, we reduce the amount of memory reads&#x2F;writes, which brings 2-4x wallclock time speedup.</p>
</blockquote>
<p><img src="/images/image-20230809113601442.png" alt="Diagram of FlashAttention forward pass: with tiling and softmax rescaling, we operate by blocks and avoid having to read/write from HBM, while obtaining the correct output with no approximation."></p>
<p>由于GPU上不同线程块和warp之间的次优工作分区，FlashAttention仍然效率低下，导致占用率低或不必要的共享内存读取&#x2F;写入</p>
<blockquote>
<p>However, FlashAttention still has some inefficiency <strong><u>due to suboptimal work partitioning between different thread blocks and warps on the GPU</u></strong>, causing either low-occupancy or unnecessary shared memory reads&#x2F;writes.</p>
</blockquote>
<h3 id="FlashAttention-2"><a href="#FlashAttention-2" class="headerlink" title="FlashAttention-2"></a>FlashAttention-2</h3><blockquote>
<p>FlashAttention-2: Better algorithm, parallelism, and work partitioning</p>
</blockquote>
<p>尽管FlashAttention在发布时已经比优化的基线快2-4倍，但它仍然有相当大的空间。FlashAttention 仍然不如优化的矩阵乘法 （GEMM） 运算快，仅达到理论最大 FLOPs&#x2F;s 的 25-40%（例如，在 A100 GPU 上高达 124 TFLOPs&#x2F;s）</p>
<p>FlashAttention-2完全从头开始重写，使用Nvidia的<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass">CUTLASS</a> 3.x及其核心库 <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md">CuTe</a>的原语，比以前的版本快约2倍，在A100 GPU（FP16 &#x2F; BF16）上达到230 TFLOP &#x2F; s。当使用端到端来训练 GPT 风格的语言模型时，我们的训练速度高达 225 TFLOP&#x2F;s（模型 FLOP 利用率为 72%）。在这篇博文中，我们描述了FlashAttention的一些瓶颈，以及我们如何利用更好的并行性和工作分区来获得显着的加速。</p>
<p><strong>Fewer non-matmul FLOPs. 更少的非矩阵 FLOP。</strong> </p>
<blockquote>
<p>我们调整了FlashAttention的算法，以减少非矩阵FLOP的数量。这很重要，因为现代GPU具有专门的计算单元（例如，Nvidia GPU上的Tensor Cores），这使得matmul更快。例如，FP16&#x2F;BF16 矩阵运算，A100 GPU 的最大理论吞吐量为 312 TFLOP&#x2F;s ，但非矩阵 FP32 的最大理论吞吐量仅为 19.5 TFLOP&#x2F;s。另一种思考方式是，每个非矩阵 FLOP 比矩阵 FLOP 贵 16 倍。为了保持高吞吐，我们希望在 matmul FLOP 上花费尽可能多的时间。</p>
<p>我们在不改输出的情况下，重写了 FlashAttention 中使用的在线 softmax 技巧（online softmax trick），以减少重新缩放操作（rescaling ops）的数量，以及边界检查（bound-checking ）和因果掩蔽操作（causal masking operations）。</p>
</blockquote>
<p><strong>Better Parallelism. 更好的并行性。</strong> </p>
<blockquote>
<p>FlashAttention 的第一个版本在batch size和head数量上并行化。我们使用 1 个线程块来处理一个注意力头，并且总体（batch_size * 个头）线程块。每个线程块都计划在流式多处理器 （SM,streaming multiprocessor） 上运行，例如，A100 GPU 上有 108 个这样的 SM。当这个数字很大（比如 &gt;&#x3D; 80）时，这种调度是有效的，因为我们可以有效地使用 GPU 上几乎所有的计算资源。</p>
<p>对于长序列（通常意味着小batch size或少量head），为了更好地利用 GPU 上的多处理器，我们现在另外在序列长度维度上进行并行化。这大大加快了这一制度的发展速度。</p>
</blockquote>
<p><strong>Better Work Partitioning. 更好的工作分区。</strong> </p>
<blockquote>
<p>即使在每个线程块中，我们还必须决定如何在不同的 warps（一组 32 个线程一起工作）之间划分工作。我们通常每个线程块使用 4 或 8 个warps，分区方案如下所述。我们在 FlashAttention-2 中改进了这种分区，以减少不同warps之间的同步和通信量(synchronization and communication)，从而减少共享内存读取&#x2F;写入。</p>
</blockquote>
<p><img src="/images/image-20230809115137501.png" alt="image-20230809115137501"></p>
<blockquote>
<p>对于每个块，FlashAttention 将 K 和 V 拆分为 4 个warps，同时保持所有warps都能访问 Q。这称为“切片 K”方案(“sliced-K”)。但是，这是低效的，因为所有warps都需要将其中间结果写入共享内存，同步，然后将中间结果相加。这些共享内存读取&#x2F;写入会减慢 FlashAttention 中的正向传递速度。</p>
<p>在 FlashAttention-2 中，我们将 Q 拆分为 4 个warps，同时保持所有warps都能访问 K 和 V。在每个warp执行矩阵乘法以获得 Q K^T 切片后，它们只需要与 V 的共享切片相乘即可获得相应的输出切片。warp之间不需要通信。共享内存读&#x2F;写的减少可加快速度。</p>
</blockquote>
<p><strong>New features: head dimensions up to 256, multi-query attention 新功能：头维度可达256，多查询注意</strong></p>
<blockquote>
<p>FlashAttention仅支持高达128的head dimensions，这适用于大多数型号，但少数被排除在外。FlashAttention-2现在支持高达256的head dimensions，这意味着GPT-J，CodeGen和CodeGen2以及StableDiffusion 1.x等型号可以使用FlashAttention-2来加速和节省内存。</p>
<p>这个新版本还支持多查询注意力 （MQA,<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.02150">multi-query attention</a>） 以及分组查询注意力 （GQA,<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.13245">grouped-query attention</a> ）。这些是attention的变体，其中多个查询头关注同一键和值头，**<u>以便在推理期间减小 KV 缓存的大小，并可以显着提高推理吞吐量。</u>**</p>
</blockquote>
<p><strong>Attention benchmark</strong></p>
<p>我们在 A100 80GB SXM4 GPU 上针对不同的设置（不带&#x2F;带因果掩码，head dimensions 64 或 128）测量不同注意力方法的运行时间。我们看到 FlashAttention-2 比 FlashAttention 快 2 倍左右（以及它在 xformers 库和 Triton 中的其他实现，使用截至 2023 年 7 月 14 日的最新开发版本）。与 PyTorch 中的标准注意力实现相比，FlashAttention-2 的速度可以提高 9 倍。</p>
<p>只需在 H100 SXM5 GPU 上运行相同的实现（不使用特殊指令来利用 TMA 和第四代Tensor Cores等新硬件功能），我们就能获得高达 335 TFLOP&#x2F;s。</p>
<p>当用于端到端训练 GPT 风格的模型时，FlashAttention-2 有助于在 A100 GPU 上实现高达 225 TFLOP&#x2F;s（模型 FLOP 利用率为 72%）。这是1.3倍的端到端加速，比已经非常优化的FlashAttention模型高。</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Day1023-The Economist"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2023/07/24/Day1023-The%20Economist/">Day1023-The Economist</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2023/07/24/Day1023-The%20Economist/" class="article-date">
	  <time datetime="2023-07-24T13:19:31.841Z" itemprop="datePublished">七月 24, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
          <div class="entry-thumbnail">
            <a href="/2023/07/24/Day1023-The%20Economist/"><img width="250" height="175" src="/images/image-20230724100544248.png" class="attachment-thumb-featured size-thumb-featured wp-post-image" alt=""></a>
          </div>
          <div class="entry-summary">
          文章来源：《经济学人》Jul 22th 2023 期 Business  栏目
Workplace advice from our agony uncle来自我们的知心大叔的职场建议
Bartleby巴托比

From hotdesking to nudity, your office questions answered从办公桌轮换到裸体，你的办公室问题得到了解答
image: paul ...

        
          <p class="article-more-link">
            <a href="/2023/07/24/Day1023-The%20Economist/#more">阅读全文</a>
          </p>
        </div>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/">经济学人</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/" rel="tag">经济学人</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Day1022-The Economist"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2023/07/23/Day1022-The%20Economist/">Day1022-The Economist</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2023/07/23/Day1022-The%20Economist/" class="article-date">
	  <time datetime="2023-07-23T04:02:16.331Z" itemprop="datePublished">七月 23, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
          <div class="entry-thumbnail">
            <a href="/2023/07/23/Day1022-The%20Economist/"><img width="250" height="175" src="/images/image-20230723114920737.png" class="attachment-thumb-featured size-thumb-featured wp-post-image" alt=""></a>
          </div>
          <div class="entry-summary">
          文章来源：《经济学人》Jul 22th 2023 期 Britain  栏目
Voters give Britain’s ruling Conservatives a historic mauling选民给英国执政的保守党一次历史性的打击
British politics英国政治

But a backlash over clean-air policies leaves questions...

        
          <p class="article-more-link">
            <a href="/2023/07/23/Day1022-The%20Economist/#more">阅读全文</a>
          </p>
        </div>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/">经济学人</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/" rel="tag">经济学人</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Day1021-The Economist"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2023/07/23/Day1021-The%20Economist/">Day1021-The Economist</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2023/07/23/Day1021-The%20Economist/" class="article-date">
	  <time datetime="2023-07-23T04:00:40.214Z" itemprop="datePublished">七月 23, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
          <div class="entry-thumbnail">
            <a href="/2023/07/23/Day1021-The%20Economist/"><img width="250" height="175" src="/images/image-20230722095156129.png" class="attachment-thumb-featured size-thumb-featured wp-post-image" alt=""></a>
          </div>
          <div class="entry-summary">
          文章来源：《经济学人》Jul 22th 2023 期 Leaders  栏目
The world economy is still in danger世界经济仍处于危险之中
Economic optimism经济乐观主义

Falling inflation is good news. But it is too early to hail a “soft landing”
“soft la...

        
          <p class="article-more-link">
            <a href="/2023/07/23/Day1021-The%20Economist/#more">阅读全文</a>
          </p>
        </div>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/">经济学人</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/" rel="tag">经济学人</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-搭建个人技术博客"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2023/07/21/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/">Hexo+Github搭建个人技术博客（MAC操作版）</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2023/07/21/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/" class="article-date">
	  <time datetime="2023-07-21T12:53:54.655Z" itemprop="datePublished">七月 21, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        

          
            <div class="entry-summary" style="margin-left:0;">
            第一次尝试搭建个人技术博客，记录踩坑实践
一、MAC本机
【1】打开终端terminal，安装homebrew
PS: 如果没有报错，可以直接安装；否则建议卸载重装
卸载命令：
1$ /bin/bash -c &quot;$(curl -fsSL https://gitee.com/ineo6/homebrew-install/raw/master/uninstall.sh)&quot;

安装命令：
1$ /bin/bash -c &quot;$(curl -fsSL https://gitee.com/ineo6/homebrew-install/raw/master/install.sh)&quot;

如果遇到“homebrew-core”相关的错误，或者运行太慢卡住，直接运行...
          

        
          <p class="article-more-link">
            <a href="/2023/07/21/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/#more">阅读全文</a>
          </p>
        </div>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%80%E5%8F%91/" rel="tag">开发</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Day1020-The Economist"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2023/07/15/Day1020-The%20Economist/">Day1020-The Economist</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2023/07/15/Day1020-The%20Economist/" class="article-date">
	  <time datetime="2023-07-15T02:34:32.647Z" itemprop="datePublished">七月 15, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
          <div class="entry-thumbnail">
            <a href="/2023/07/15/Day1020-The%20Economist/"><img width="250" height="175" src="/images/image-20230721193603150.png" class="attachment-thumb-featured size-thumb-featured wp-post-image" alt=""></a>
          </div>
          <div class="entry-summary">
          文章来源：《经济学人》Jul 22th 2023 期 Leaders  栏目
Making babymaking betterFertility technology
IVF is failing most women. But new research holds out hope for the future试管受精让大多数女性失望。但新的研究为未来带来了希望
Jul 20th 2023...

        
          <p class="article-more-link">
            <a href="/2023/07/15/Day1020-The%20Economist/#more">阅读全文</a>
          </p>
        </div>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/">经济学人</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/" rel="tag">经济学人</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Day1019-The Economist"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2023/07/15/Day1019-The%20Economist/">Day1019-The Economist</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2023/07/15/Day1019-The%20Economist/" class="article-date">
	  <time datetime="2023-07-15T02:34:28.298Z" itemprop="datePublished">七月 15, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
          <div class="entry-thumbnail">
            <a href="/2023/07/15/Day1019-The%20Economist/"><img width="250" height="175" src="/images/image-20230718085522402.png" class="attachment-thumb-featured size-thumb-featured wp-post-image" alt=""></a>
          </div>
          <div class="entry-summary">
          文章来源：《经济学人》Jul 15th 2023 期 Leaders  栏目
Big pharma is warming to the potential of AI大型制药公司开始关注人工智能的潜力
Wonder drugs特效药

But some worry the Terminator is coming但是有些人担心终结者就要来了
Jul 13th 202
image: Bryan...

        
          <p class="article-more-link">
            <a href="/2023/07/15/Day1019-The%20Economist/#more">阅读全文</a>
          </p>
        </div>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/">经济学人</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/" rel="tag">经济学人</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Day1018-The Economist"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2023/07/15/Day1018-The%20Economist/">Day1018-The Economist</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2023/07/15/Day1018-The%20Economist/" class="article-date">
	  <time datetime="2023-07-15T02:34:24.675Z" itemprop="datePublished">七月 15, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
          <div class="entry-thumbnail">
            <a href="/2023/07/15/Day1018-The%20Economist/"><img width="250" height="175" src="/images/image-20230716102725323.png" class="attachment-thumb-featured size-thumb-featured wp-post-image" alt=""></a>
          </div>
          <div class="entry-summary">
          文章来源：《经济学人》Jul 15th 2023 期 Leaders  栏目
How MAGA Republicans plan to make Donald Trump’s second term countMAGA共和党人计划让唐纳德·特朗普的第二个任期有所成就
Preparing for government
They think they know how to banish the...

        
          <p class="article-more-link">
            <a href="/2023/07/15/Day1018-The%20Economist/#more">阅读全文</a>
          </p>
        </div>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/">经济学人</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/" rel="tag">经济学人</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Day1017-The Economist"  class="article article-type-post" itemscope itemprop="blogPost" >
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2023/07/03/Day1017-The%20Economist/">Day1017-The Economist</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2023/07/03/Day1017-The%20Economist/" class="article-date">
	  <time datetime="2023-07-03T01:45:06.394Z" itemprop="datePublished">七月 3, 2023</time>
	</a>

      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        
          <div class="entry-thumbnail">
            <a href="/2023/07/03/Day1017-The%20Economist/"><img width="250" height="175" src="/images/image-20230713210728159.png" class="attachment-thumb-featured size-thumb-featured wp-post-image" alt=""></a>
          </div>
          <div class="entry-summary">
          文章来源：《经济学人》Jul 15th 2023 期 Business  栏目
Is big business really getting too big?大企业真的会越变越大吗？
Size wars规模战

In a few sectors, corporate concentration is a problem. In most, it needn’t be
在一些行业，企业“高集中...

        
          <p class="article-more-link">
            <a href="/2023/07/03/Day1017-The%20Economist/#more">阅读全文</a>
          </p>
        </div>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/">经济学人</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/" rel="tag">经济学人</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>

</section>
          <aside id="sidebar">
  
    <div class="widget-wrap" style="margin: 20px 0;">
	<div id="search-form-wrap">

    <form class="search-form">
        <label style="width: 75%;">
            <span class="screen-reader-text">Search for:</span>
            <input type="search" class="search-field" style="height: 42px;" placeholder=" 搜索…" value="" name="s" title="Search for:">
        </label>
        <input type="submit" class="search-form-submit" value="搜索">
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="请输入关键词..."/>
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(无标题)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
</div>
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">联系我们</h3>
    <div class="widget widget_athemes_social_icons">

    	<ul class="clearfix widget-social-icons">   
    	
          
            <li><a href="mailto:yjchen@hnu.edu.cn?subject=请联系我&body=我能帮你什么" title="email"><i class="fa fa-envelope" aria-hidden="true"></i></a></li> 
          
   		
   		</ul>


   		<!--
   		<ul class="clearfix widget-social-icons">   		
   		<li class="widget-si-twitter"><a target="_blank" rel="noopener" href="http://twitter.com" title="Twitter"><i class="ico-twitter"></i></a></li> 
		<li class="widget-si-facebook"><a target="_blank" rel="noopener" href="http://facebook.com" title="Facebook"><i class="ico-facebook"></i></a></li>
			<li class="widget-si-gplus"><a target="_blank" rel="noopener" href="http://plus.google.com" title="Google+"><i class="ico-gplus"></i></a></li>
			<li class="widget-si-pinterest"><a target="_blank" rel="noopener" href="http://pinterest.com" title="Pinterest"><i class="ico-pinterest"></i></a></li>
			<li class="widget-si-flickr"><a target="_blank" rel="noopener" href="http://flickr.com" title="Flickr"><i class="ico-flickr"></i></a></li>
			<li class="widget-si-instagram"><a target="_blank" rel="noopener" href="http://instagram.com" title="Instagram"><i class="ico-instagram"></i></a></li>
		</ul> -->

    </div>
  </div>


  
    
  <div class="widget_athemes_tabs">
    <ul id="widget-tab" class="clearfix widget-tab-nav">
      <li class="active"><a>最新文章</a></li>
    </ul>
    <div class="widget">
      <ul>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2023/08/09/Untitled/">(no title)</a></h6>
              <span>八月 9, 2023</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2023/08/08/LLM%E7%9B%B8%E5%85%B3%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95/">LLM相关进展</a></h6>
              <span>八月 8, 2023</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-thumbnail">
                <a href="/2023/07/24/Day1023-The%20Economist/" title=""><img width="50" height="50" src="/images/image-20230724100544248.png" class="attachment-thumb-small size-thumb-small wp-post-image" alt="preview-16" title=""></a>
              </div>
              <div class="widget-entry-summary">
            

              <h6 style="margin: 0;"><a href="/2023/07/24/Day1023-The%20Economist/">Day1023-The Economist</a></h6>
              <span>七月 24, 2023</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-thumbnail">
                <a href="/2023/07/23/Day1022-The%20Economist/" title=""><img width="50" height="50" src="/images/image-20230723114920737.png" class="attachment-thumb-small size-thumb-small wp-post-image" alt="preview-16" title=""></a>
              </div>
              <div class="widget-entry-summary">
            

              <h6 style="margin: 0;"><a href="/2023/07/23/Day1022-The%20Economist/">Day1022-The Economist</a></h6>
              <span>七月 23, 2023</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-thumbnail">
                <a href="/2023/07/23/Day1021-The%20Economist/" title=""><img width="50" height="50" src="/images/image-20230722095156129.png" class="attachment-thumb-small size-thumb-small wp-post-image" alt="preview-16" title=""></a>
              </div>
              <div class="widget-entry-summary">
            

              <h6 style="margin: 0;"><a href="/2023/07/23/Day1021-The%20Economist/">Day1021-The Economist</a></h6>
              <span>七月 23, 2023</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2023/07/21/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2/">Hexo+Github搭建个人技术博客（MAC操作版）</a></h6>
              <span>七月 21, 2023</span>
            </div>

          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/">经济学人</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/">计算机</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%80%E5%8F%91/" rel="tag">开发</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E4%BA%BA/" rel="tag">经济学人</a><span class="tag-list-count">9</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>

    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a><span class="archive-list-count">10</span></li></ul>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2023 Yujie&#39;s Blog All Rights Reserved.
          
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->

<script src="/js/my.js"></script>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.css">

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.js"></script>




<script src="/js/scripts.js"></script>


<script src="https://stackpath.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>


<script src="/js/main.js"></script>









	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
